Loading graph from: /home/markoa/workspace/data/ProstateCancer/go-basic.obo
Graph loaded: 40214 nodes, 77600 edges.

Pruning graph to include only specified root nodes: 2
Keeping 36191 nodes connected to specified root nodes.
Removing 4023 nodes not connected to specified root nodes.
Pruning complete.
Nodes: 40214 -> 36191 (4023 removed)
Edges: 77600 -> 71188 (6412 removed)

Calculating hierarchy levels...
Found 2 root nodes (terms with no children):
• GO:0003674: molecular_function
• GO:0008150: biological_process
Processing 2 nodes at level 0
Processing 19436 nodes at level 5
Processing 7602 nodes at level 10
Processing 367 nodes at level 15
Hierarchy calculation complete. Max level: 18
Nodes per hierarchy level:
  Level 0: 2 nodes
  Level 1: 49 nodes
  Level 2: 289 nodes
  Level 3: 1099 nodes
  Level 4: 2682 nodes
  Level 5: 6239 nodes
  Level 6: 5094 nodes
  Level 7: 4764 nodes
  Level 8: 4398 nodes
  Level 9: 3807 nodes
  Level 10: 3054 nodes
  Level 11: 2153 nodes
  Level 12: 1267 nodes
  Level 13: 611 nodes
  Level 14: 316 nodes
  Level 15: 195 nodes
  Level 16: 109 nodes
  Level 17: 58 nodes
  Level 18: 5 nodes

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 36191
Layer start indices: [0, 2, 51, 340, 1439, 4121, 10360, 15454, 20218, 24616, 28423, 31477, 33630, 34897, 35508, 35824, 36019, 36128, 36186, 36191]

Example indices:
 Level 0:
  • GO:0003674 (Index 0): molecular_function
  • GO:0008150 (Index 1): biological_process
 Level 1:
  • GO:0002376 (Index 2): immune system process
  • GO:0003774 (Index 3): cytoskeletal motor activity
  • GO:0003824 (Index 4): catalytic activity
 Level 2:
  • GO:0000156 (Index 51): phosphorelay response regulator activity
  • GO:0000995 (Index 52): RNA polymerase III general transcription initiation factor activity
  • GO:0001070 (Index 53): RNA-binding transcription regulator activity

Pruning graph to max hierarchy level 10...
Removing 4714 nodes.
Pruning complete.
Nodes: 36191 -> 31477 (4714 removed)
Edges: 71188 -> 58696 (12492 removed)

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 31477
Layer start indices: [0, 2, 51, 340, 1439, 4121, 10360, 15454, 20218, 24616, 28423, 31477]

Example indices:
 Level 0:
  • GO:0003674 (Index 0): molecular_function
  • GO:0008150 (Index 1): biological_process
 Level 1:
  • GO:0002376 (Index 2): immune system process
  • GO:0003774 (Index 3): cytoskeletal motor activity
  • GO:0003824 (Index 4): catalytic activity
 Level 2:
  • GO:0000156 (Index 51): phosphorelay response regulator activity
  • GO:0000995 (Index 52): RNA polymerase III general transcription initiation factor activity
  • GO:0001070 (Index 53): RNA-binding transcription regulator activity
Sequential indices and edge index recalculated for the pruned graph.
Available columns in GAF DataFrame:
['DB', 'DB_Object_ID', 'DB_Object_Symbol', 'Qualifier', 'GO_ID', 'DB_Reference', 'Evidence_Code', 'With_From', 'Aspect', 'DB_Object_Name', 'DB_Object_Synonym', 'DB_Object_Type', 'Taxon', 'Date', 'Assigned_By', 'Annotation_Extension', 'Gene_Product_Form_ID', 'Replacement_GO_ID']

Adding protein layer starting at index 31477
Added 8911 proteins to the graph
Created 130021 protein-GO term edges
Skipped 150902 annotations with GO terms not in the graph

Calculating excluded proteins (UniProt IDs)...
Initial intended UniProt IDs list size: 9131
UniProt IDs added to graph layer: 8911
UniProt IDs excluded (intended but not added): 220
Found 8911 protein nodes at hierarchy level 11
Starting forward traversal from protein nodes...
Original graph: 40388 nodes, 188717 edges
Pruned graph: 23908 nodes, 158716 edges
Removed 16480 unreachable nodes (40.8% of total)

Nodes by hierarchy level:
Level    Before   Removed  After    % Removed
---------------------------------------------
Level 0  2        0        2        0.0%
Level 1  49       6        43       12.2%
Level 2  289      93       196      32.2%
Level 3  1099     375      724      34.1%
Level 4  2682     1266     1416     47.2%
Level 5  6239     3940     2299     63.2%
Level 6  5094     2643     2451     51.9%
Level 7  4764     2402     2362     50.4%
Level 8  4398     2214     2184     50.3%
Level 9  3807     1974     1833     51.9%
Level 10 3054     1567     1487     51.3%
Protein  8911     0        8911     0.0%

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 23908
Layer start indices: [0, 2, 45, 241, 965, 2381, 4680, 7131, 9493, 11677, 13510, 14997, 23908]

Example indices:
 Level 0:
  • GO:0003674 (Index 0): molecular_function
  • GO:0008150 (Index 1): biological_process
 Level 1:
  • GO:0002376 (Index 2): immune system process
  • GO:0003774 (Index 3): cytoskeletal motor activity
  • GO:0003824 (Index 4): catalytic activity
 Level 2:
  • GO:0000995 (Index 45): RNA polymerase III general transcription initiation factor activity
  • GO:0001503 (Index 46): ossification
  • GO:0001775 (Index 47): cell activation

Removing first layer (level 0) from the graph...
Removed 7 protein nodes connected only to level 0.
Number of protein nodes after removal: 8904
Original graph: 23906 nodes, 158606 edges
Pruned graph: 23899 nodes, 158606 edges

Nodes by hierarchy level after removal:
Level    After
--------------------
Level 0  43
Level 1  196
Level 2  724
Level 3  1416
Level 4  2299
Level 5  2451
Level 6  2362
Level 7  2184
Level 8  1833
Level 9  1487
Protein  8904

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 23899
Layer start indices: [0, 43, 239, 963, 2379, 4678, 7129, 9491, 11675, 13508, 14995, 23899]

Example indices:
 Level 0:
  • GO:0002376 (Index 0): immune system process
  • GO:0003774 (Index 1): cytoskeletal motor activity
  • GO:0003824 (Index 2): catalytic activity
 Level 1:
  • GO:0000995 (Index 43): RNA polymerase III general transcription initiation factor activity
  • GO:0001503 (Index 44): ossification
  • GO:0001775 (Index 45): cell activation
 Level 2:
  • GO:0000035 (Index 239): acyl binding
  • GO:0000149 (Index 240): SNARE binding
  • GO:0000278 (Index 241): mitotic cell cycle
Removed 2 nodes from level 0.
New max hierarchy level: 10
Sequential indices recalculated.

Creating edge index matrix...
Edge index created. Total edges in index: 158606

Example edges (source_idx -> target_idx):
  [4678 -> 2822] : (GO:0000002 -> GO:0007005)
  [4679 -> 2380] : (GO:0000009 -> GO:0000030)
  [9491 -> 7439] : (GO:0000012 -> GO:0006281)
  [7129 -> 4922] : (GO:0000014 -> GO:0004520)
  [2379 -> 1077] : (GO:0000016 -> GO:0004553)
Shape of X: (1012, 9131)
Count of input genes: 9131
Shape of X: (1012, 9131)
Count of input genes: 9131
Shape of X_cnv: (1012, 9131)
Shape of y_cnv: (1012, 1)
Number of input genes used: 9131
Number of features to KEEP based on PathwayNetwork additions: 8904
Number of normalized features to KEEP: 8904
Extracted 9131 IDs from column 'gene' in features_cnv DataFrame.
Number of features kept using 'added' set (with normalization): 8904
Shape of aligned X_cnv: (1012, 8904)
X_train shape: (809, 8904), y_train shape: (809, 1)
X_test shape: (203, 8904), y_test shape: (203, 1)
Train dataset size: 809, Test dataset size: 203
Number of train batches: 102, Number of test batches: 26
Using device: cuda
Epoch [1/50], Batch [10/102], Avg Loss (last 10): 0.6931
Epoch [1/50], Batch [20/102], Avg Loss (last 10): 0.6931
Epoch [1/50], Batch [30/102], Avg Loss (last 10): 0.6930
Epoch [1/50], Batch [40/102], Avg Loss (last 10): 0.6930
Epoch [1/50], Batch [50/102], Avg Loss (last 10): 0.6929
Epoch [1/50], Batch [60/102], Avg Loss (last 10): 0.6930
Epoch [1/50], Batch [70/102], Avg Loss (last 10): 0.6928
Epoch [1/50], Batch [80/102], Avg Loss (last 10): 0.6930
Epoch [1/50], Batch [90/102], Avg Loss (last 10): 0.6925
Epoch [1/50], Batch [100/102], Avg Loss (last 10): 0.6927
Epoch [1/50] Average Training Loss: 0.6929
/home/markoa/workspace/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch [2/50], Batch [10/102], Avg Loss (last 10): 0.6924
Epoch [2/50], Batch [20/102], Avg Loss (last 10): 0.6927
Epoch [2/50], Batch [30/102], Avg Loss (last 10): 0.6926
Epoch [2/50], Batch [40/102], Avg Loss (last 10): 0.6925
Epoch [2/50], Batch [50/102], Avg Loss (last 10): 0.6923
Epoch [2/50], Batch [60/102], Avg Loss (last 10): 0.6925
Epoch [2/50], Batch [70/102], Avg Loss (last 10): 0.6922
Epoch [2/50], Batch [80/102], Avg Loss (last 10): 0.6924
Epoch [2/50], Batch [90/102], Avg Loss (last 10): 0.6918
Epoch [2/50], Batch [100/102], Avg Loss (last 10): 0.6923
Epoch [2/50] Average Training Loss: 0.6924
Epoch [3/50], Batch [10/102], Avg Loss (last 10): 0.6922
Epoch [3/50], Batch [20/102], Avg Loss (last 10): 0.6919
Epoch [3/50], Batch [30/102], Avg Loss (last 10): 0.6920
Epoch [3/50], Batch [40/102], Avg Loss (last 10): 0.6925
Epoch [3/50], Batch [50/102], Avg Loss (last 10): 0.6918
Epoch [3/50], Batch [60/102], Avg Loss (last 10): 0.6920
Epoch [3/50], Batch [70/102], Avg Loss (last 10): 0.6923
Epoch [3/50], Batch [80/102], Avg Loss (last 10): 0.6913
Epoch [3/50], Batch [90/102], Avg Loss (last 10): 0.6913
Epoch [3/50], Batch [100/102], Avg Loss (last 10): 0.6917
Epoch [3/50] Average Training Loss: 0.6919
Epoch [4/50], Batch [10/102], Avg Loss (last 10): 0.6918
Epoch [4/50], Batch [20/102], Avg Loss (last 10): 0.6917
Epoch [4/50], Batch [30/102], Avg Loss (last 10): 0.6922
Epoch [4/50], Batch [40/102], Avg Loss (last 10): 0.6913
Epoch [4/50], Batch [50/102], Avg Loss (last 10): 0.6910
Epoch [4/50], Batch [60/102], Avg Loss (last 10): 0.6910
Epoch [4/50], Batch [70/102], Avg Loss (last 10): 0.6917
Epoch [4/50], Batch [80/102], Avg Loss (last 10): 0.6912
Epoch [4/50], Batch [90/102], Avg Loss (last 10): 0.6910
Epoch [4/50], Batch [100/102], Avg Loss (last 10): 0.6908
Epoch [4/50] Average Training Loss: 0.6914
Epoch [5/50], Batch [10/102], Avg Loss (last 10): 0.6914
Epoch [5/50], Batch [20/102], Avg Loss (last 10): 0.6899
Epoch [5/50], Batch [30/102], Avg Loss (last 10): 0.6909
Epoch [5/50], Batch [40/102], Avg Loss (last 10): 0.6914
Epoch [5/50], Batch [50/102], Avg Loss (last 10): 0.6913
Epoch [5/50], Batch [60/102], Avg Loss (last 10): 0.6903
Epoch [5/50], Batch [70/102], Avg Loss (last 10): 0.6919
Epoch [5/50], Batch [80/102], Avg Loss (last 10): 0.6907
Epoch [5/50], Batch [90/102], Avg Loss (last 10): 0.6910
Epoch [5/50], Batch [100/102], Avg Loss (last 10): 0.6899
Epoch [5/50] Average Training Loss: 0.6908
Epoch [6/50], Batch [10/102], Avg Loss (last 10): 0.6902
Epoch [6/50], Batch [20/102], Avg Loss (last 10): 0.6905
Epoch [6/50], Batch [30/102], Avg Loss (last 10): 0.6920
Epoch [6/50], Batch [40/102], Avg Loss (last 10): 0.6908
Epoch [6/50], Batch [50/102], Avg Loss (last 10): 0.6902
Epoch [6/50], Batch [60/102], Avg Loss (last 10): 0.6905
Epoch [6/50], Batch [70/102], Avg Loss (last 10): 0.6896
Epoch [6/50], Batch [80/102], Avg Loss (last 10): 0.6900
Epoch [6/50], Batch [90/102], Avg Loss (last 10): 0.6897
Epoch [6/50], Batch [100/102], Avg Loss (last 10): 0.6899
Epoch [6/50] Average Training Loss: 0.6905
Epoch [7/50], Batch [10/102], Avg Loss (last 10): 0.6891
Epoch [7/50], Batch [20/102], Avg Loss (last 10): 0.6914
Epoch [7/50], Batch [30/102], Avg Loss (last 10): 0.6902
Epoch [7/50], Batch [40/102], Avg Loss (last 10): 0.6902
Epoch [7/50], Batch [50/102], Avg Loss (last 10): 0.6903
Epoch [7/50], Batch [60/102], Avg Loss (last 10): 0.6894
Epoch [7/50], Batch [70/102], Avg Loss (last 10): 0.6891
Epoch [7/50], Batch [80/102], Avg Loss (last 10): 0.6905
Epoch [7/50], Batch [90/102], Avg Loss (last 10): 0.6897
Epoch [7/50], Batch [100/102], Avg Loss (last 10): 0.6889
Epoch [7/50] Average Training Loss: 0.6900
Epoch [8/50], Batch [10/102], Avg Loss (last 10): 0.6888
Epoch [8/50], Batch [20/102], Avg Loss (last 10): 0.6903
Epoch [8/50], Batch [30/102], Avg Loss (last 10): 0.6890
Epoch [8/50], Batch [40/102], Avg Loss (last 10): 0.6908
Epoch [8/50], Batch [50/102], Avg Loss (last 10): 0.6891
Epoch [8/50], Batch [60/102], Avg Loss (last 10): 0.6896
Epoch [8/50], Batch [70/102], Avg Loss (last 10): 0.6882
Epoch [8/50], Batch [80/102], Avg Loss (last 10): 0.6898
Epoch [8/50], Batch [90/102], Avg Loss (last 10): 0.6878
Epoch [8/50], Batch [100/102], Avg Loss (last 10): 0.6915
Epoch [8/50] Average Training Loss: 0.6894
Epoch [9/50], Batch [10/102], Avg Loss (last 10): 0.6876
Epoch [9/50], Batch [20/102], Avg Loss (last 10): 0.6923
Epoch [9/50], Batch [30/102], Avg Loss (last 10): 0.6887
Epoch [9/50], Batch [40/102], Avg Loss (last 10): 0.6886
Epoch [9/50], Batch [50/102], Avg Loss (last 10): 0.6880
Epoch [9/50], Batch [60/102], Avg Loss (last 10): 0.6891
Epoch [9/50], Batch [70/102], Avg Loss (last 10): 0.6878
Epoch [9/50], Batch [80/102], Avg Loss (last 10): 0.6894
Epoch [9/50], Batch [90/102], Avg Loss (last 10): 0.6887
Epoch [9/50], Batch [100/102], Avg Loss (last 10): 0.6883
Epoch [9/50] Average Training Loss: 0.6891
Epoch [10/50], Batch [10/102], Avg Loss (last 10): 0.6899
Epoch [10/50], Batch [20/102], Avg Loss (last 10): 0.6879
Epoch [10/50], Batch [30/102], Avg Loss (last 10): 0.6871
Epoch [10/50], Batch [40/102], Avg Loss (last 10): 0.6891
Epoch [10/50], Batch [50/102], Avg Loss (last 10): 0.6891
Epoch [10/50], Batch [60/102], Avg Loss (last 10): 0.6873
Epoch [10/50], Batch [70/102], Avg Loss (last 10): 0.6872
Epoch [10/50], Batch [80/102], Avg Loss (last 10): 0.6886
Epoch [10/50], Batch [90/102], Avg Loss (last 10): 0.6904
Epoch [10/50], Batch [100/102], Avg Loss (last 10): 0.6878
Epoch [10/50] Average Training Loss: 0.6886
Epoch [11/50], Batch [10/102], Avg Loss (last 10): 0.6888
Epoch [11/50], Batch [20/102], Avg Loss (last 10): 0.6907
Epoch [11/50], Batch [30/102], Avg Loss (last 10): 0.6899
Epoch [11/50], Batch [40/102], Avg Loss (last 10): 0.6872
Epoch [11/50], Batch [50/102], Avg Loss (last 10): 0.6883
Epoch [11/50], Batch [60/102], Avg Loss (last 10): 0.6875
Epoch [11/50], Batch [70/102], Avg Loss (last 10): 0.6863
Epoch [11/50], Batch [80/102], Avg Loss (last 10): 0.6894
Epoch [11/50], Batch [90/102], Avg Loss (last 10): 0.6862
Epoch [11/50], Batch [100/102], Avg Loss (last 10): 0.6861
Epoch [11/50] Average Training Loss: 0.6879
Epoch [12/50], Batch [10/102], Avg Loss (last 10): 0.6872
Epoch [12/50], Batch [20/102], Avg Loss (last 10): 0.6880
Epoch [12/50], Batch [30/102], Avg Loss (last 10): 0.6851
Epoch [12/50], Batch [40/102], Avg Loss (last 10): 0.6866
Epoch [12/50], Batch [50/102], Avg Loss (last 10): 0.6899
Epoch [12/50], Batch [60/102], Avg Loss (last 10): 0.6882
Epoch [12/50], Batch [70/102], Avg Loss (last 10): 0.6882
Epoch [12/50], Batch [80/102], Avg Loss (last 10): 0.6873
Epoch [12/50], Batch [90/102], Avg Loss (last 10): 0.6855
Epoch [12/50], Batch [100/102], Avg Loss (last 10): 0.6889
Epoch [12/50] Average Training Loss: 0.6874
Epoch [13/50], Batch [10/102], Avg Loss (last 10): 0.6876
Epoch [13/50], Batch [20/102], Avg Loss (last 10): 0.6871
Epoch [13/50], Batch [30/102], Avg Loss (last 10): 0.6862
Epoch [13/50], Batch [40/102], Avg Loss (last 10): 0.6843
Epoch [13/50], Batch [50/102], Avg Loss (last 10): 0.6874
Epoch [13/50], Batch [60/102], Avg Loss (last 10): 0.6878
Epoch [13/50], Batch [70/102], Avg Loss (last 10): 0.6882
Epoch [13/50], Batch [80/102], Avg Loss (last 10): 0.6859
Epoch [13/50], Batch [90/102], Avg Loss (last 10): 0.6891
Epoch [13/50], Batch [100/102], Avg Loss (last 10): 0.6886
Epoch [13/50] Average Training Loss: 0.6870
Epoch [14/50], Batch [10/102], Avg Loss (last 10): 0.6876
Epoch [14/50], Batch [20/102], Avg Loss (last 10): 0.6842
Epoch [14/50], Batch [30/102], Avg Loss (last 10): 0.6827
Epoch [14/50], Batch [40/102], Avg Loss (last 10): 0.6890
Epoch [14/50], Batch [50/102], Avg Loss (last 10): 0.6845
Epoch [14/50], Batch [60/102], Avg Loss (last 10): 0.6879
Epoch [14/50], Batch [70/102], Avg Loss (last 10): 0.6844
Epoch [14/50], Batch [80/102], Avg Loss (last 10): 0.6878
Epoch [14/50], Batch [90/102], Avg Loss (last 10): 0.6903
Epoch [14/50], Batch [100/102], Avg Loss (last 10): 0.6867
Epoch [14/50] Average Training Loss: 0.6869
Epoch [15/50], Batch [10/102], Avg Loss (last 10): 0.6877
Epoch [15/50], Batch [20/102], Avg Loss (last 10): 0.6856
Epoch [15/50], Batch [30/102], Avg Loss (last 10): 0.6856
Epoch [15/50], Batch [40/102], Avg Loss (last 10): 0.6887
Epoch [15/50], Batch [50/102], Avg Loss (last 10): 0.6860
Epoch [15/50], Batch [60/102], Avg Loss (last 10): 0.6855
Epoch [15/50], Batch [70/102], Avg Loss (last 10): 0.6859
Epoch [15/50], Batch [80/102], Avg Loss (last 10): 0.6822
Epoch [15/50], Batch [90/102], Avg Loss (last 10): 0.6891
Epoch [15/50], Batch [100/102], Avg Loss (last 10): 0.6853
Epoch [15/50] Average Training Loss: 0.6860
Epoch [16/50], Batch [10/102], Avg Loss (last 10): 0.6825
Epoch [16/50], Batch [20/102], Avg Loss (last 10): 0.6868
Epoch [16/50], Batch [30/102], Avg Loss (last 10): 0.6829
Epoch [16/50], Batch [40/102], Avg Loss (last 10): 0.6889
Epoch [16/50], Batch [50/102], Avg Loss (last 10): 0.6855
Epoch [16/50], Batch [60/102], Avg Loss (last 10): 0.6866
Epoch [16/50], Batch [70/102], Avg Loss (last 10): 0.6854
Epoch [16/50], Batch [80/102], Avg Loss (last 10): 0.6865
Epoch [16/50], Batch [90/102], Avg Loss (last 10): 0.6859
Epoch [16/50], Batch [100/102], Avg Loss (last 10): 0.6859
Epoch [16/50] Average Training Loss: 0.6856
Epoch [17/50], Batch [10/102], Avg Loss (last 10): 0.6882
Epoch [17/50], Batch [20/102], Avg Loss (last 10): 0.6846
Epoch [17/50], Batch [30/102], Avg Loss (last 10): 0.6811
Epoch [17/50], Batch [40/102], Avg Loss (last 10): 0.6828
Epoch [17/50], Batch [50/102], Avg Loss (last 10): 0.6863
Epoch [17/50], Batch [60/102], Avg Loss (last 10): 0.6850
Epoch [17/50], Batch [70/102], Avg Loss (last 10): 0.6820
Epoch [17/50], Batch [80/102], Avg Loss (last 10): 0.6892
Epoch [17/50], Batch [90/102], Avg Loss (last 10): 0.6861
Epoch [17/50], Batch [100/102], Avg Loss (last 10): 0.6873
Epoch [17/50] Average Training Loss: 0.6851
Epoch [18/50], Batch [10/102], Avg Loss (last 10): 0.6854
Epoch [18/50], Batch [20/102], Avg Loss (last 10): 0.6848
Epoch [18/50], Batch [30/102], Avg Loss (last 10): 0.6816
Epoch [18/50], Batch [40/102], Avg Loss (last 10): 0.6859
Epoch [18/50], Batch [50/102], Avg Loss (last 10): 0.6846
Epoch [18/50], Batch [60/102], Avg Loss (last 10): 0.6865
Epoch [18/50], Batch [70/102], Avg Loss (last 10): 0.6884
Epoch [18/50], Batch [80/102], Avg Loss (last 10): 0.6858
Epoch [18/50], Batch [90/102], Avg Loss (last 10): 0.6812
Epoch [18/50], Batch [100/102], Avg Loss (last 10): 0.6838
Epoch [18/50] Average Training Loss: 0.6851
Epoch [19/50], Batch [10/102], Avg Loss (last 10): 0.6791
Epoch [19/50], Batch [20/102], Avg Loss (last 10): 0.6836
Epoch [19/50], Batch [30/102], Avg Loss (last 10): 0.6836
Epoch [19/50], Batch [40/102], Avg Loss (last 10): 0.6835
Epoch [19/50], Batch [50/102], Avg Loss (last 10): 0.6841
Epoch [19/50], Batch [60/102], Avg Loss (last 10): 0.6848
Epoch [19/50], Batch [70/102], Avg Loss (last 10): 0.6834
Epoch [19/50], Batch [80/102], Avg Loss (last 10): 0.6860
Epoch [19/50], Batch [90/102], Avg Loss (last 10): 0.6894
Epoch [19/50], Batch [100/102], Avg Loss (last 10): 0.6846
Epoch [19/50] Average Training Loss: 0.6842
Epoch [20/50], Batch [10/102], Avg Loss (last 10): 0.6873
Epoch [20/50], Batch [20/102], Avg Loss (last 10): 0.6852
Epoch [20/50], Batch [30/102], Avg Loss (last 10): 0.6817
Epoch [20/50], Batch [40/102], Avg Loss (last 10): 0.6810
Epoch [20/50], Batch [50/102], Avg Loss (last 10): 0.6858
Epoch [20/50], Batch [60/102], Avg Loss (last 10): 0.6858
Epoch [20/50], Batch [70/102], Avg Loss (last 10): 0.6872
Epoch [20/50], Batch [80/102], Avg Loss (last 10): 0.6815
Epoch [20/50], Batch [90/102], Avg Loss (last 10): 0.6793
Epoch [20/50], Batch [100/102], Avg Loss (last 10): 0.6849
Epoch [20/50] Average Training Loss: 0.6837
Epoch [21/50], Batch [10/102], Avg Loss (last 10): 0.6834
Epoch [21/50], Batch [20/102], Avg Loss (last 10): 0.6863
Epoch [21/50], Batch [30/102], Avg Loss (last 10): 0.6790
Epoch [21/50], Batch [40/102], Avg Loss (last 10): 0.6833
Epoch [21/50], Batch [50/102], Avg Loss (last 10): 0.6825
Epoch [21/50], Batch [60/102], Avg Loss (last 10): 0.6884
Epoch [21/50], Batch [70/102], Avg Loss (last 10): 0.6832
Epoch [21/50], Batch [80/102], Avg Loss (last 10): 0.6854
Epoch [21/50], Batch [90/102], Avg Loss (last 10): 0.6831
Epoch [21/50], Batch [100/102], Avg Loss (last 10): 0.6793
Epoch [21/50] Average Training Loss: 0.6838
Epoch [22/50], Batch [10/102], Avg Loss (last 10): 0.6792
Epoch [22/50], Batch [20/102], Avg Loss (last 10): 0.6845
Epoch [22/50], Batch [30/102], Avg Loss (last 10): 0.6806
Epoch [22/50], Batch [40/102], Avg Loss (last 10): 0.6875
Epoch [22/50], Batch [50/102], Avg Loss (last 10): 0.6843
Epoch [22/50], Batch [60/102], Avg Loss (last 10): 0.6835
Epoch [22/50], Batch [70/102], Avg Loss (last 10): 0.6851
Epoch [22/50], Batch [80/102], Avg Loss (last 10): 0.6811
Epoch [22/50], Batch [90/102], Avg Loss (last 10): 0.6834
Epoch [22/50], Batch [100/102], Avg Loss (last 10): 0.6818
Epoch [22/50] Average Training Loss: 0.6828
Epoch [23/50], Batch [10/102], Avg Loss (last 10): 0.6817
Epoch [23/50], Batch [20/102], Avg Loss (last 10): 0.6809
Epoch [23/50], Batch [30/102], Avg Loss (last 10): 0.6913
Epoch [23/50], Batch [40/102], Avg Loss (last 10): 0.6816
Epoch [23/50], Batch [50/102], Avg Loss (last 10): 0.6799
Epoch [23/50], Batch [60/102], Avg Loss (last 10): 0.6815
Epoch [23/50], Batch [70/102], Avg Loss (last 10): 0.6782
Epoch [23/50], Batch [80/102], Avg Loss (last 10): 0.6888
Epoch [23/50], Batch [90/102], Avg Loss (last 10): 0.6789
Epoch [23/50], Batch [100/102], Avg Loss (last 10): 0.6830
Epoch [23/50] Average Training Loss: 0.6824
Epoch [24/50], Batch [10/102], Avg Loss (last 10): 0.6879
Epoch [24/50], Batch [20/102], Avg Loss (last 10): 0.6787
Epoch [24/50], Batch [30/102], Avg Loss (last 10): 0.6820
Epoch [24/50], Batch [40/102], Avg Loss (last 10): 0.6828
Epoch [24/50], Batch [50/102], Avg Loss (last 10): 0.6853
Epoch [24/50], Batch [60/102], Avg Loss (last 10): 0.6819
Epoch [24/50], Batch [70/102], Avg Loss (last 10): 0.6810
Epoch [24/50], Batch [80/102], Avg Loss (last 10): 0.6818
Epoch [24/50], Batch [90/102], Avg Loss (last 10): 0.6783
Epoch [24/50], Batch [100/102], Avg Loss (last 10): 0.6808
Epoch [24/50] Average Training Loss: 0.6825
Epoch [25/50], Batch [10/102], Avg Loss (last 10): 0.6764
Epoch [25/50], Batch [20/102], Avg Loss (last 10): 0.6842
Epoch [25/50], Batch [30/102], Avg Loss (last 10): 0.6824
Epoch [25/50], Batch [40/102], Avg Loss (last 10): 0.6832
Epoch [25/50], Batch [50/102], Avg Loss (last 10): 0.6823
Epoch [25/50], Batch [60/102], Avg Loss (last 10): 0.6849
Epoch [25/50], Batch [70/102], Avg Loss (last 10): 0.6813
Epoch [25/50], Batch [80/102], Avg Loss (last 10): 0.6813
Epoch [25/50], Batch [90/102], Avg Loss (last 10): 0.6786
Epoch [25/50], Batch [100/102], Avg Loss (last 10): 0.6812
Epoch [25/50] Average Training Loss: 0.6815
Epoch [26/50], Batch [10/102], Avg Loss (last 10): 0.6803
Epoch [26/50], Batch [20/102], Avg Loss (last 10): 0.6857
Epoch [26/50], Batch [30/102], Avg Loss (last 10): 0.6784
Epoch [26/50], Batch [40/102], Avg Loss (last 10): 0.6820
Epoch [26/50], Batch [50/102], Avg Loss (last 10): 0.6810
Epoch [26/50], Batch [60/102], Avg Loss (last 10): 0.6782
Epoch [26/50], Batch [70/102], Avg Loss (last 10): 0.6883
Epoch [26/50], Batch [80/102], Avg Loss (last 10): 0.6781
Epoch [26/50], Batch [90/102], Avg Loss (last 10): 0.6827
Epoch [26/50], Batch [100/102], Avg Loss (last 10): 0.6771
Epoch [26/50] Average Training Loss: 0.6810
Epoch [27/50], Batch [10/102], Avg Loss (last 10): 0.6863
Epoch [27/50], Batch [20/102], Avg Loss (last 10): 0.6760
Epoch [27/50], Batch [30/102], Avg Loss (last 10): 0.6825
Epoch [27/50], Batch [40/102], Avg Loss (last 10): 0.6825
Epoch [27/50], Batch [50/102], Avg Loss (last 10): 0.6796
Epoch [27/50], Batch [60/102], Avg Loss (last 10): 0.6786
Epoch [27/50], Batch [70/102], Avg Loss (last 10): 0.6805
Epoch [27/50], Batch [80/102], Avg Loss (last 10): 0.6804
Epoch [27/50], Batch [90/102], Avg Loss (last 10): 0.6784
Epoch [27/50], Batch [100/102], Avg Loss (last 10): 0.6832
Epoch [27/50] Average Training Loss: 0.6813
Epoch [28/50], Batch [10/102], Avg Loss (last 10): 0.6793
Epoch [28/50], Batch [20/102], Avg Loss (last 10): 0.6812
Epoch [28/50], Batch [30/102], Avg Loss (last 10): 0.6763
Epoch [28/50], Batch [40/102], Avg Loss (last 10): 0.6762
Epoch [28/50], Batch [50/102], Avg Loss (last 10): 0.6781
Epoch [28/50], Batch [60/102], Avg Loss (last 10): 0.6860
Epoch [28/50], Batch [70/102], Avg Loss (last 10): 0.6750
Epoch [28/50], Batch [80/102], Avg Loss (last 10): 0.6829
Epoch [28/50], Batch [90/102], Avg Loss (last 10): 0.6819
Epoch [28/50], Batch [100/102], Avg Loss (last 10): 0.6859
Epoch [28/50] Average Training Loss: 0.6809
Epoch [29/50], Batch [10/102], Avg Loss (last 10): 0.6829
Epoch [29/50], Batch [20/102], Avg Loss (last 10): 0.6818
Epoch [29/50], Batch [30/102], Avg Loss (last 10): 0.6798
Epoch [29/50], Batch [40/102], Avg Loss (last 10): 0.6818
Epoch [29/50], Batch [50/102], Avg Loss (last 10): 0.6766
Epoch [29/50], Batch [60/102], Avg Loss (last 10): 0.6827
Epoch [29/50], Batch [70/102], Avg Loss (last 10): 0.6796
Epoch [29/50], Batch [80/102], Avg Loss (last 10): 0.6796
Epoch [29/50], Batch [90/102], Avg Loss (last 10): 0.6754
Epoch [29/50], Batch [100/102], Avg Loss (last 10): 0.6805
Epoch [29/50] Average Training Loss: 0.6797
Epoch [30/50], Batch [10/102], Avg Loss (last 10): 0.6846
Epoch [30/50], Batch [20/102], Avg Loss (last 10): 0.6836
Epoch [30/50], Batch [30/102], Avg Loss (last 10): 0.6720
Epoch [30/50], Batch [40/102], Avg Loss (last 10): 0.6793
Epoch [30/50], Batch [50/102], Avg Loss (last 10): 0.6771
Epoch [30/50], Batch [60/102], Avg Loss (last 10): 0.6813
Epoch [30/50], Batch [70/102], Avg Loss (last 10): 0.6813
Epoch [30/50], Batch [80/102], Avg Loss (last 10): 0.6738
Epoch [30/50], Batch [90/102], Avg Loss (last 10): 0.6769
Epoch [30/50], Batch [100/102], Avg Loss (last 10): 0.6865
Epoch [30/50] Average Training Loss: 0.6800
Epoch [31/50], Batch [10/102], Avg Loss (last 10): 0.6822
Epoch [31/50], Batch [20/102], Avg Loss (last 10): 0.6811
Epoch [31/50], Batch [30/102], Avg Loss (last 10): 0.6843
Epoch [31/50], Batch [40/102], Avg Loss (last 10): 0.6746
Epoch [31/50], Batch [50/102], Avg Loss (last 10): 0.6767
Epoch [31/50], Batch [60/102], Avg Loss (last 10): 0.6788
Epoch [31/50], Batch [70/102], Avg Loss (last 10): 0.6799
Epoch [31/50], Batch [80/102], Avg Loss (last 10): 0.6831
Epoch [31/50], Batch [90/102], Avg Loss (last 10): 0.6765
Epoch [31/50], Batch [100/102], Avg Loss (last 10): 0.6764
Epoch [31/50] Average Training Loss: 0.6789
Epoch [32/50], Batch [10/102], Avg Loss (last 10): 0.6808
Epoch [32/50], Batch [20/102], Avg Loss (last 10): 0.6741
Epoch [32/50], Batch [30/102], Avg Loss (last 10): 0.6774
Epoch [32/50], Batch [40/102], Avg Loss (last 10): 0.6762
Epoch [32/50], Batch [50/102], Avg Loss (last 10): 0.6806
Epoch [32/50], Batch [60/102], Avg Loss (last 10): 0.6783
Epoch [32/50], Batch [70/102], Avg Loss (last 10): 0.6874
Epoch [32/50], Batch [80/102], Avg Loss (last 10): 0.6703
Epoch [32/50], Batch [90/102], Avg Loss (last 10): 0.6737
Epoch [32/50], Batch [100/102], Avg Loss (last 10): 0.6873
Epoch [32/50] Average Training Loss: 0.6785
Epoch [33/50], Batch [10/102], Avg Loss (last 10): 0.6804
Epoch [33/50], Batch [20/102], Avg Loss (last 10): 0.6689
Epoch [33/50], Batch [30/102], Avg Loss (last 10): 0.6769
Epoch [33/50], Batch [40/102], Avg Loss (last 10): 0.6745
Epoch [33/50], Batch [50/102], Avg Loss (last 10): 0.6745
Epoch [33/50], Batch [60/102], Avg Loss (last 10): 0.6837
Epoch [33/50], Batch [70/102], Avg Loss (last 10): 0.6849
Epoch [33/50], Batch [80/102], Avg Loss (last 10): 0.6767
Epoch [33/50], Batch [90/102], Avg Loss (last 10): 0.6778
Epoch [33/50], Batch [100/102], Avg Loss (last 10): 0.6813
Epoch [33/50] Average Training Loss: 0.6780
Epoch [34/50], Batch [10/102], Avg Loss (last 10): 0.6836
Epoch [34/50], Batch [20/102], Avg Loss (last 10): 0.6812
Epoch [34/50], Batch [30/102], Avg Loss (last 10): 0.6705
Epoch [34/50], Batch [40/102], Avg Loss (last 10): 0.6824
Epoch [34/50], Batch [50/102], Avg Loss (last 10): 0.6799
Epoch [34/50], Batch [60/102], Avg Loss (last 10): 0.6751
Epoch [34/50], Batch [70/102], Avg Loss (last 10): 0.6739
Epoch [34/50], Batch [80/102], Avg Loss (last 10): 0.6738
Epoch [34/50], Batch [90/102], Avg Loss (last 10): 0.6883
Epoch [34/50], Batch [100/102], Avg Loss (last 10): 0.6713
Epoch [34/50] Average Training Loss: 0.6776
Epoch [35/50], Batch [10/102], Avg Loss (last 10): 0.6688
Epoch [35/50], Batch [20/102], Avg Loss (last 10): 0.6785
Epoch [35/50], Batch [30/102], Avg Loss (last 10): 0.6735
Epoch [35/50], Batch [40/102], Avg Loss (last 10): 0.6845
Epoch [35/50], Batch [50/102], Avg Loss (last 10): 0.6783
Epoch [35/50], Batch [60/102], Avg Loss (last 10): 0.6857
Epoch [35/50], Batch [70/102], Avg Loss (last 10): 0.6733
Epoch [35/50], Batch [80/102], Avg Loss (last 10): 0.6820
Epoch [35/50], Batch [90/102], Avg Loss (last 10): 0.6720
Epoch [35/50], Batch [100/102], Avg Loss (last 10): 0.6757
Epoch [35/50] Average Training Loss: 0.6781
Epoch [36/50], Batch [10/102], Avg Loss (last 10): 0.6794
Epoch [36/50], Batch [20/102], Avg Loss (last 10): 0.6831
Epoch [36/50], Batch [30/102], Avg Loss (last 10): 0.6743
Epoch [36/50], Batch [40/102], Avg Loss (last 10): 0.6780
Epoch [36/50], Batch [50/102], Avg Loss (last 10): 0.6780
Epoch [36/50], Batch [60/102], Avg Loss (last 10): 0.6805
Epoch [36/50], Batch [70/102], Avg Loss (last 10): 0.6703
Epoch [36/50], Batch [80/102], Avg Loss (last 10): 0.6804
Epoch [36/50], Batch [90/102], Avg Loss (last 10): 0.6715
Epoch [36/50], Batch [100/102], Avg Loss (last 10): 0.6752
Epoch [36/50] Average Training Loss: 0.6777
Epoch [37/50], Batch [10/102], Avg Loss (last 10): 0.6816
Epoch [37/50], Batch [20/102], Avg Loss (last 10): 0.6713
Epoch [37/50], Batch [30/102], Avg Loss (last 10): 0.6738
Epoch [37/50], Batch [40/102], Avg Loss (last 10): 0.6777
Epoch [37/50], Batch [50/102], Avg Loss (last 10): 0.6828
Epoch [37/50], Batch [60/102], Avg Loss (last 10): 0.6802
Epoch [37/50], Batch [70/102], Avg Loss (last 10): 0.6736
Epoch [37/50], Batch [80/102], Avg Loss (last 10): 0.6814
Epoch [37/50], Batch [90/102], Avg Loss (last 10): 0.6735
Epoch [37/50], Batch [100/102], Avg Loss (last 10): 0.6722
Epoch [37/50] Average Training Loss: 0.6764
Epoch [38/50], Batch [10/102], Avg Loss (last 10): 0.6734
Epoch [38/50], Batch [20/102], Avg Loss (last 10): 0.6747
Epoch [38/50], Batch [30/102], Avg Loss (last 10): 0.6773
Epoch [38/50], Batch [40/102], Avg Loss (last 10): 0.6826
Epoch [38/50], Batch [50/102], Avg Loss (last 10): 0.6786
Epoch [38/50], Batch [60/102], Avg Loss (last 10): 0.6812
Epoch [38/50], Batch [70/102], Avg Loss (last 10): 0.6745
Epoch [38/50], Batch [80/102], Avg Loss (last 10): 0.6691
Epoch [38/50], Batch [90/102], Avg Loss (last 10): 0.6798
Epoch [38/50], Batch [100/102], Avg Loss (last 10): 0.6703
Epoch [38/50] Average Training Loss: 0.6769
Epoch [39/50], Batch [10/102], Avg Loss (last 10): 0.6756
Epoch [39/50], Batch [20/102], Avg Loss (last 10): 0.6892
Epoch [39/50], Batch [30/102], Avg Loss (last 10): 0.6701
Epoch [39/50], Batch [40/102], Avg Loss (last 10): 0.6810
Epoch [39/50], Batch [50/102], Avg Loss (last 10): 0.6659
Epoch [39/50], Batch [60/102], Avg Loss (last 10): 0.6809
Epoch [39/50], Batch [70/102], Avg Loss (last 10): 0.6713
Epoch [39/50], Batch [80/102], Avg Loss (last 10): 0.6726
Epoch [39/50], Batch [90/102], Avg Loss (last 10): 0.6767
Epoch [39/50], Batch [100/102], Avg Loss (last 10): 0.6739
Epoch [39/50] Average Training Loss: 0.6765
Epoch [40/50], Batch [10/102], Avg Loss (last 10): 0.6711
Epoch [40/50], Batch [20/102], Avg Loss (last 10): 0.6669
Epoch [40/50], Batch [30/102], Avg Loss (last 10): 0.6696
Epoch [40/50], Batch [40/102], Avg Loss (last 10): 0.6737
Epoch [40/50], Batch [50/102], Avg Loss (last 10): 0.6863
Epoch [40/50], Batch [60/102], Avg Loss (last 10): 0.6736
Epoch [40/50], Batch [70/102], Avg Loss (last 10): 0.6820
Epoch [40/50], Batch [80/102], Avg Loss (last 10): 0.6764
Epoch [40/50], Batch [90/102], Avg Loss (last 10): 0.6778
Epoch [40/50], Batch [100/102], Avg Loss (last 10): 0.6749
Epoch [40/50] Average Training Loss: 0.6762
Epoch [41/50], Batch [10/102], Avg Loss (last 10): 0.6777
Epoch [41/50], Batch [20/102], Avg Loss (last 10): 0.6706
Epoch [41/50], Batch [30/102], Avg Loss (last 10): 0.6691
Epoch [41/50], Batch [40/102], Avg Loss (last 10): 0.6776
Epoch [41/50], Batch [50/102], Avg Loss (last 10): 0.6747
Epoch [41/50], Batch [60/102], Avg Loss (last 10): 0.6718
Epoch [41/50], Batch [70/102], Avg Loss (last 10): 0.6804
Epoch [41/50], Batch [80/102], Avg Loss (last 10): 0.6703
Epoch [41/50], Batch [90/102], Avg Loss (last 10): 0.6818
Epoch [41/50], Batch [100/102], Avg Loss (last 10): 0.6760
Epoch [41/50] Average Training Loss: 0.6748
Epoch [42/50], Batch [10/102], Avg Loss (last 10): 0.6788
Epoch [42/50], Batch [20/102], Avg Loss (last 10): 0.6715
Epoch [42/50], Batch [30/102], Avg Loss (last 10): 0.6788
Epoch [42/50], Batch [40/102], Avg Loss (last 10): 0.6685
Epoch [42/50], Batch [50/102], Avg Loss (last 10): 0.6787
Epoch [42/50], Batch [60/102], Avg Loss (last 10): 0.6640
Epoch [42/50], Batch [70/102], Avg Loss (last 10): 0.6742
Epoch [42/50], Batch [80/102], Avg Loss (last 10): 0.6786
Epoch [42/50], Batch [90/102], Avg Loss (last 10): 0.6741
Epoch [42/50], Batch [100/102], Avg Loss (last 10): 0.6786
Epoch [42/50] Average Training Loss: 0.6744
Epoch [43/50], Batch [10/102], Avg Loss (last 10): 0.6651
Epoch [43/50], Batch [20/102], Avg Loss (last 10): 0.6710
Epoch [43/50], Batch [30/102], Avg Loss (last 10): 0.6620
Epoch [43/50], Batch [40/102], Avg Loss (last 10): 0.6814
Epoch [43/50], Batch [50/102], Avg Loss (last 10): 0.6739
Epoch [43/50], Batch [60/102], Avg Loss (last 10): 0.6769
Epoch [43/50], Batch [70/102], Avg Loss (last 10): 0.6799
Epoch [43/50], Batch [80/102], Avg Loss (last 10): 0.6844
Epoch [43/50], Batch [90/102], Avg Loss (last 10): 0.6813
Epoch [43/50], Batch [100/102], Avg Loss (last 10): 0.6676
Epoch [43/50] Average Training Loss: 0.6740
Epoch [44/50], Batch [10/102], Avg Loss (last 10): 0.6706
Epoch [44/50], Batch [20/102], Avg Loss (last 10): 0.6767
Epoch [44/50], Batch [30/102], Avg Loss (last 10): 0.6782
Epoch [44/50], Batch [40/102], Avg Loss (last 10): 0.6705
Epoch [44/50], Batch [50/102], Avg Loss (last 10): 0.6797
Epoch [44/50], Batch [60/102], Avg Loss (last 10): 0.6766
Epoch [44/50], Batch [70/102], Avg Loss (last 10): 0.6673
Epoch [44/50], Batch [80/102], Avg Loss (last 10): 0.6657
Epoch [44/50], Batch [90/102], Avg Loss (last 10): 0.6718
Epoch [44/50], Batch [100/102], Avg Loss (last 10): 0.6842
Epoch [44/50] Average Training Loss: 0.6736
Epoch [45/50], Batch [10/102], Avg Loss (last 10): 0.6764
Epoch [45/50], Batch [20/102], Avg Loss (last 10): 0.6748
Epoch [45/50], Batch [30/102], Avg Loss (last 10): 0.6701
Epoch [45/50], Batch [40/102], Avg Loss (last 10): 0.6779
Epoch [45/50], Batch [50/102], Avg Loss (last 10): 0.6700
Epoch [45/50], Batch [60/102], Avg Loss (last 10): 0.6794
Epoch [45/50], Batch [70/102], Avg Loss (last 10): 0.6715
Epoch [45/50], Batch [80/102], Avg Loss (last 10): 0.6715
Epoch [45/50], Batch [90/102], Avg Loss (last 10): 0.6730
Epoch [45/50], Batch [100/102], Avg Loss (last 10): 0.6714
Epoch [45/50] Average Training Loss: 0.6732
Epoch [46/50], Batch [10/102], Avg Loss (last 10): 0.6713
Epoch [46/50], Batch [20/102], Avg Loss (last 10): 0.6729
Epoch [46/50], Batch [30/102], Avg Loss (last 10): 0.6776
Epoch [46/50], Batch [40/102], Avg Loss (last 10): 0.6568
Epoch [46/50], Batch [50/102], Avg Loss (last 10): 0.6776
Epoch [46/50], Batch [60/102], Avg Loss (last 10): 0.6775
Epoch [46/50], Batch [70/102], Avg Loss (last 10): 0.6759
Epoch [46/50], Batch [80/102], Avg Loss (last 10): 0.6710
Epoch [46/50], Batch [90/102], Avg Loss (last 10): 0.6807
Epoch [46/50], Batch [100/102], Avg Loss (last 10): 0.6694
Epoch [46/50] Average Training Loss: 0.6740
Epoch [47/50], Batch [10/102], Avg Loss (last 10): 0.6677
Epoch [47/50], Batch [20/102], Avg Loss (last 10): 0.6644
Epoch [47/50], Batch [30/102], Avg Loss (last 10): 0.6660
Epoch [47/50], Batch [40/102], Avg Loss (last 10): 0.6806
Epoch [47/50], Batch [50/102], Avg Loss (last 10): 0.6740
Epoch [47/50], Batch [60/102], Avg Loss (last 10): 0.6691
Epoch [47/50], Batch [70/102], Avg Loss (last 10): 0.6723
Epoch [47/50], Batch [80/102], Avg Loss (last 10): 0.6871
Epoch [47/50], Batch [90/102], Avg Loss (last 10): 0.6706
Epoch [47/50], Batch [100/102], Avg Loss (last 10): 0.6805
Epoch [47/50] Average Training Loss: 0.6725
Epoch [48/50], Batch [10/102], Avg Loss (last 10): 0.6655
Epoch [48/50], Batch [20/102], Avg Loss (last 10): 0.6771
Epoch [48/50], Batch [30/102], Avg Loss (last 10): 0.6671
Epoch [48/50], Batch [40/102], Avg Loss (last 10): 0.6737
Epoch [48/50], Batch [50/102], Avg Loss (last 10): 0.6754
Epoch [48/50], Batch [60/102], Avg Loss (last 10): 0.6954
Epoch [48/50], Batch [70/102], Avg Loss (last 10): 0.6820
Epoch [48/50], Batch [80/102], Avg Loss (last 10): 0.6535
Epoch [48/50], Batch [90/102], Avg Loss (last 10): 0.6685
Epoch [48/50], Batch [100/102], Avg Loss (last 10): 0.6685
Epoch [48/50] Average Training Loss: 0.6721
Epoch [49/50], Batch [10/102], Avg Loss (last 10): 0.6684
Epoch [49/50], Batch [20/102], Avg Loss (last 10): 0.6921
Epoch [49/50], Batch [30/102], Avg Loss (last 10): 0.6734
Epoch [49/50], Batch [40/102], Avg Loss (last 10): 0.6649
Epoch [49/50], Batch [50/102], Avg Loss (last 10): 0.6751
Epoch [49/50], Batch [60/102], Avg Loss (last 10): 0.6716
Epoch [49/50], Batch [70/102], Avg Loss (last 10): 0.6716
Epoch [49/50], Batch [80/102], Avg Loss (last 10): 0.6647
Epoch [49/50], Batch [90/102], Avg Loss (last 10): 0.6681
Epoch [49/50], Batch [100/102], Avg Loss (last 10): 0.6749
Epoch [49/50] Average Training Loss: 0.6718
Epoch [50/50], Batch [10/102], Avg Loss (last 10): 0.6869
Epoch [50/50], Batch [20/102], Avg Loss (last 10): 0.6714
Epoch [50/50], Batch [30/102], Avg Loss (last 10): 0.6679
Epoch [50/50], Batch [40/102], Avg Loss (last 10): 0.6644
Epoch [50/50], Batch [50/102], Avg Loss (last 10): 0.6609
Epoch [50/50], Batch [60/102], Avg Loss (last 10): 0.6712
Epoch [50/50], Batch [70/102], Avg Loss (last 10): 0.6677
Epoch [50/50], Batch [80/102], Avg Loss (last 10): 0.6746
Epoch [50/50], Batch [90/102], Avg Loss (last 10): 0.6833
Epoch [50/50], Batch [100/102], Avg Loss (last 10): 0.6693
Epoch [50/50] Average Training Loss: 0.6714
