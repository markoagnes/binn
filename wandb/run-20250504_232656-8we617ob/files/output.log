Loading graph from: /home/markoa/workspace/data/ProstateCancer/go-basic.obo
Graph loaded: 40214 nodes, 77600 edges.

Pruning graph to include only specified root nodes: 2
Keeping 36191 nodes connected to specified root nodes.
Removing 4023 nodes not connected to specified root nodes.
Pruning complete.
Nodes: 40214 -> 36191 (4023 removed)
Edges: 77600 -> 71188 (6412 removed)

Calculating hierarchy levels...
Found 2 root nodes (terms with no children):
• GO:0003674: molecular_function
• GO:0008150: biological_process
Processing 2 nodes at level 0
Processing 19436 nodes at level 5
Processing 7602 nodes at level 10
Processing 367 nodes at level 15
Hierarchy calculation complete. Max level: 18
Nodes per hierarchy level:
  Level 0: 2 nodes
  Level 1: 49 nodes
  Level 2: 289 nodes
  Level 3: 1099 nodes
  Level 4: 2682 nodes
  Level 5: 6239 nodes
  Level 6: 5094 nodes
  Level 7: 4764 nodes
  Level 8: 4398 nodes
  Level 9: 3807 nodes
  Level 10: 3054 nodes
  Level 11: 2153 nodes
  Level 12: 1267 nodes
  Level 13: 611 nodes
  Level 14: 316 nodes
  Level 15: 195 nodes
  Level 16: 109 nodes
  Level 17: 58 nodes
  Level 18: 5 nodes

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 36191
Layer start indices: [0, 2, 51, 340, 1439, 4121, 10360, 15454, 20218, 24616, 28423, 31477, 33630, 34897, 35508, 35824, 36019, 36128, 36186, 36191]

Example indices:
 Level 0:
  • GO:0003674 (Index 0): molecular_function
  • GO:0008150 (Index 1): biological_process
 Level 1:
  • GO:0002376 (Index 2): immune system process
  • GO:0003774 (Index 3): cytoskeletal motor activity
  • GO:0003824 (Index 4): catalytic activity
 Level 2:
  • GO:0000156 (Index 51): phosphorelay response regulator activity
  • GO:0000995 (Index 52): RNA polymerase III general transcription initiation factor activity
  • GO:0001070 (Index 53): RNA-binding transcription regulator activity

Pruning graph to max hierarchy level 10...
Removing 4714 nodes.
Pruning complete.
Nodes: 36191 -> 31477 (4714 removed)
Edges: 71188 -> 58696 (12492 removed)

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 31477
Layer start indices: [0, 2, 51, 340, 1439, 4121, 10360, 15454, 20218, 24616, 28423, 31477]

Example indices:
 Level 0:
  • GO:0003674 (Index 0): molecular_function
  • GO:0008150 (Index 1): biological_process
 Level 1:
  • GO:0002376 (Index 2): immune system process
  • GO:0003774 (Index 3): cytoskeletal motor activity
  • GO:0003824 (Index 4): catalytic activity
 Level 2:
  • GO:0000156 (Index 51): phosphorelay response regulator activity
  • GO:0000995 (Index 52): RNA polymerase III general transcription initiation factor activity
  • GO:0001070 (Index 53): RNA-binding transcription regulator activity
Sequential indices and edge index recalculated for the pruned graph.
Available columns in GAF DataFrame:
['DB', 'DB_Object_ID', 'DB_Object_Symbol', 'Qualifier', 'GO_ID', 'DB_Reference', 'Evidence_Code', 'With_From', 'Aspect', 'DB_Object_Name', 'DB_Object_Synonym', 'DB_Object_Type', 'Taxon', 'Date', 'Assigned_By', 'Annotation_Extension', 'Gene_Product_Form_ID', 'Replacement_GO_ID']

Adding protein layer starting at index 31477
Added 8911 proteins to the graph
Created 130021 protein-GO term edges
Skipped 150902 annotations with GO terms not in the graph

Calculating excluded proteins (UniProt IDs)...
Initial intended UniProt IDs list size: 9131
UniProt IDs added to graph layer: 8911
UniProt IDs excluded (intended but not added): 220
Found 8911 protein nodes at hierarchy level 11
Starting forward traversal from protein nodes...
Original graph: 40388 nodes, 188717 edges
Pruned graph: 23908 nodes, 158716 edges
Removed 16480 unreachable nodes (40.8% of total)

Nodes by hierarchy level:
Level    Before   Removed  After    % Removed
---------------------------------------------
Level 0  2        0        2        0.0%
Level 1  49       6        43       12.2%
Level 2  289      93       196      32.2%
Level 3  1099     375      724      34.1%
Level 4  2682     1266     1416     47.2%
Level 5  6239     3940     2299     63.2%
Level 6  5094     2643     2451     51.9%
Level 7  4764     2402     2362     50.4%
Level 8  4398     2214     2184     50.3%
Level 9  3807     1974     1833     51.9%
Level 10 3054     1567     1487     51.3%
Protein  8911     0        8911     0.0%

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 23908
Layer start indices: [0, 2, 45, 241, 965, 2381, 4680, 7131, 9493, 11677, 13510, 14997, 23908]

Example indices:
 Level 0:
  • GO:0003674 (Index 0): molecular_function
  • GO:0008150 (Index 1): biological_process
 Level 1:
  • GO:0002376 (Index 2): immune system process
  • GO:0003774 (Index 3): cytoskeletal motor activity
  • GO:0003824 (Index 4): catalytic activity
 Level 2:
  • GO:0000995 (Index 45): RNA polymerase III general transcription initiation factor activity
  • GO:0001503 (Index 46): ossification
  • GO:0001775 (Index 47): cell activation

Removing first layer (level 0) from the graph...
Removed 7 protein nodes connected only to level 0.
Number of protein nodes after removal: 8904
Original graph: 23906 nodes, 158606 edges
Pruned graph: 23899 nodes, 158606 edges

Nodes by hierarchy level after removal:
Level    After
--------------------
Level 0  43
Level 1  196
Level 2  724
Level 3  1416
Level 4  2299
Level 5  2451
Level 6  2362
Level 7  2184
Level 8  1833
Level 9  1487
Protein  8904

Calculating sequential indices...
Sequential indices assigned. Total indexed nodes: 23899
Layer start indices: [0, 43, 239, 963, 2379, 4678, 7129, 9491, 11675, 13508, 14995, 23899]

Example indices:
 Level 0:
  • GO:0002376 (Index 0): immune system process
  • GO:0003774 (Index 1): cytoskeletal motor activity
  • GO:0003824 (Index 2): catalytic activity
 Level 1:
  • GO:0000995 (Index 43): RNA polymerase III general transcription initiation factor activity
  • GO:0001503 (Index 44): ossification
  • GO:0001775 (Index 45): cell activation
 Level 2:
  • GO:0000035 (Index 239): acyl binding
  • GO:0000149 (Index 240): SNARE binding
  • GO:0000278 (Index 241): mitotic cell cycle
Removed 2 nodes from level 0.
New max hierarchy level: 10
Sequential indices recalculated.

Creating edge index matrix...
Edge index created. Total edges in index: 158606

Example edges (source_idx -> target_idx):
  [4678 -> 2822] : (GO:0000002 -> GO:0007005)
  [4679 -> 2380] : (GO:0000009 -> GO:0000030)
  [9491 -> 7439] : (GO:0000012 -> GO:0006281)
  [7129 -> 4922] : (GO:0000014 -> GO:0004520)
  [2379 -> 1077] : (GO:0000016 -> GO:0004553)
Shape of X: (1012, 9131)
Count of input genes: 9131
Shape of X: (1012, 9131)
Count of input genes: 9131
Shape of X_cnv: (1012, 9131)
Shape of y_cnv: (1012, 1)
Number of input genes used: 9131
Number of features to KEEP based on PathwayNetwork additions: 8904
Number of normalized features to KEEP: 8904
Extracted 9131 IDs from column 'gene' in features_cnv DataFrame.
Number of features kept using 'added' set (with normalization): 8904
Shape of aligned X_cnv: (1012, 8904)
X_train shape: (809, 8904), y_train shape: (809, 1)
X_test shape: (203, 8904), y_test shape: (203, 1)
Train dataset size: 809, Test dataset size: 203
Number of train batches: 102, Number of test batches: 26
Using device: cuda
/home/markoa/workspace/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Epoch [1/50], Batch [10/102], Avg Loss (last 10): 0.6931
Epoch [1/50], Batch [20/102], Avg Loss (last 10): 0.6912
Epoch [1/50], Batch [30/102], Avg Loss (last 10): 0.6895
Epoch [1/50], Batch [40/102], Avg Loss (last 10): 0.6911
Epoch [1/50], Batch [50/102], Avg Loss (last 10): 0.6881
Epoch [1/50], Batch [60/102], Avg Loss (last 10): 0.6892
Epoch [1/50], Batch [70/102], Avg Loss (last 10): 0.6887
Epoch [1/50], Batch [80/102], Avg Loss (last 10): 0.6851
Epoch [1/50], Batch [90/102], Avg Loss (last 10): 0.6772
Epoch [1/50], Batch [100/102], Avg Loss (last 10): 0.6818
Epoch [1/50] Average Training Loss: 0.6872
Epoch [2/50], Batch [10/102], Avg Loss (last 10): 0.6857
Epoch [2/50], Batch [20/102], Avg Loss (last 10): 0.6841
Epoch [2/50], Batch [30/102], Avg Loss (last 10): 0.6823
Epoch [2/50], Batch [40/102], Avg Loss (last 10): 0.6773
Epoch [2/50], Batch [50/102], Avg Loss (last 10): 0.6712
Epoch [2/50], Batch [60/102], Avg Loss (last 10): 0.6750
Epoch [2/50], Batch [70/102], Avg Loss (last 10): 0.6764
Epoch [2/50], Batch [80/102], Avg Loss (last 10): 0.6769
Epoch [2/50], Batch [90/102], Avg Loss (last 10): 0.6763
Epoch [2/50], Batch [100/102], Avg Loss (last 10): 0.6607
Epoch [2/50] Average Training Loss: 0.6773
Epoch [3/50], Batch [10/102], Avg Loss (last 10): 0.6745
Epoch [3/50], Batch [20/102], Avg Loss (last 10): 0.6626
Epoch [3/50], Batch [30/102], Avg Loss (last 10): 0.6654
Epoch [3/50], Batch [40/102], Avg Loss (last 10): 0.6762
Epoch [3/50], Batch [50/102], Avg Loss (last 10): 0.6695
Epoch [3/50], Batch [60/102], Avg Loss (last 10): 0.6732
Epoch [3/50], Batch [70/102], Avg Loss (last 10): 0.6506
Epoch [3/50], Batch [80/102], Avg Loss (last 10): 0.6742
Epoch [3/50], Batch [90/102], Avg Loss (last 10): 0.6619
Epoch [3/50], Batch [100/102], Avg Loss (last 10): 0.6709
Epoch [3/50] Average Training Loss: 0.6690
Epoch [4/50], Batch [10/102], Avg Loss (last 10): 0.6628
Epoch [4/50], Batch [20/102], Avg Loss (last 10): 0.6647
Epoch [4/50], Batch [30/102], Avg Loss (last 10): 0.6696
Epoch [4/50], Batch [40/102], Avg Loss (last 10): 0.6580
Epoch [4/50], Batch [50/102], Avg Loss (last 10): 0.6716
Epoch [4/50], Batch [60/102], Avg Loss (last 10): 0.6566
Epoch [4/50], Batch [70/102], Avg Loss (last 10): 0.6440
Epoch [4/50], Batch [80/102], Avg Loss (last 10): 0.6613
Epoch [4/50], Batch [90/102], Avg Loss (last 10): 0.6604
Epoch [4/50], Batch [100/102], Avg Loss (last 10): 0.6469
Epoch [4/50] Average Training Loss: 0.6620
Epoch [5/50], Batch [10/102], Avg Loss (last 10): 0.6592
Epoch [5/50], Batch [20/102], Avg Loss (last 10): 0.6588
Epoch [5/50], Batch [30/102], Avg Loss (last 10): 0.6514
Epoch [5/50], Batch [40/102], Avg Loss (last 10): 0.6405
Epoch [5/50], Batch [50/102], Avg Loss (last 10): 0.6536
Epoch [5/50], Batch [60/102], Avg Loss (last 10): 0.6637
Epoch [5/50], Batch [70/102], Avg Loss (last 10): 0.6597
Epoch [5/50], Batch [80/102], Avg Loss (last 10): 0.6557
Epoch [5/50], Batch [90/102], Avg Loss (last 10): 0.6208
Epoch [5/50], Batch [100/102], Avg Loss (last 10): 0.6741
Epoch [5/50] Average Training Loss: 0.6536
Epoch [6/50], Batch [10/102], Avg Loss (last 10): 0.6739
Epoch [6/50], Batch [20/102], Avg Loss (last 10): 0.6380
Epoch [6/50], Batch [30/102], Avg Loss (last 10): 0.6127
Epoch [6/50], Batch [40/102], Avg Loss (last 10): 0.6779
Epoch [6/50], Batch [50/102], Avg Loss (last 10): 0.6441
Epoch [6/50], Batch [60/102], Avg Loss (last 10): 0.6393
Epoch [6/50], Batch [70/102], Avg Loss (last 10): 0.6821
Epoch [6/50], Batch [80/102], Avg Loss (last 10): 0.6647
Epoch [6/50], Batch [90/102], Avg Loss (last 10): 0.6294
Epoch [6/50], Batch [100/102], Avg Loss (last 10): 0.6465
Epoch [6/50] Average Training Loss: 0.6485
Epoch [7/50], Batch [10/102], Avg Loss (last 10): 0.6505
Epoch [7/50], Batch [20/102], Avg Loss (last 10): 0.6130
Epoch [7/50], Batch [30/102], Avg Loss (last 10): 0.6308
Epoch [7/50], Batch [40/102], Avg Loss (last 10): 0.6348
Epoch [7/50], Batch [50/102], Avg Loss (last 10): 0.6390
Epoch [7/50], Batch [60/102], Avg Loss (last 10): 0.6829
Epoch [7/50], Batch [70/102], Avg Loss (last 10): 0.6681
Epoch [7/50], Batch [80/102], Avg Loss (last 10): 0.6382
Epoch [7/50], Batch [90/102], Avg Loss (last 10): 0.6228
Epoch [7/50], Batch [100/102], Avg Loss (last 10): 0.6730
Epoch [7/50] Average Training Loss: 0.6446
Epoch [8/50], Batch [10/102], Avg Loss (last 10): 0.6319
Epoch [8/50], Batch [20/102], Avg Loss (last 10): 0.6575
Epoch [8/50], Batch [30/102], Avg Loss (last 10): 0.6678
Epoch [8/50], Batch [40/102], Avg Loss (last 10): 0.6204
Epoch [8/50], Batch [50/102], Avg Loss (last 10): 0.6411
Epoch [8/50], Batch [60/102], Avg Loss (last 10): 0.6300
Epoch [8/50], Batch [70/102], Avg Loss (last 10): 0.6624
Epoch [8/50], Batch [80/102], Avg Loss (last 10): 0.6293
Epoch [8/50], Batch [90/102], Avg Loss (last 10): 0.6621
Epoch [8/50], Batch [100/102], Avg Loss (last 10): 0.6231
Epoch [8/50] Average Training Loss: 0.6413
Epoch [9/50], Batch [10/102], Avg Loss (last 10): 0.6393
Epoch [9/50], Batch [20/102], Avg Loss (last 10): 0.6563
Epoch [9/50], Batch [30/102], Avg Loss (last 10): 0.6733
Epoch [9/50], Batch [40/102], Avg Loss (last 10): 0.6331
Epoch [9/50], Batch [50/102], Avg Loss (last 10): 0.6096
Epoch [9/50], Batch [60/102], Avg Loss (last 10): 0.6383
Epoch [9/50], Batch [70/102], Avg Loss (last 10): 0.6558
Epoch [9/50], Batch [80/102], Avg Loss (last 10): 0.6617
Epoch [9/50], Batch [90/102], Avg Loss (last 10): 0.6138
Epoch [9/50], Batch [100/102], Avg Loss (last 10): 0.6314
Epoch [9/50] Average Training Loss: 0.6387
Epoch [10/50], Batch [10/102], Avg Loss (last 10): 0.5882
Epoch [10/50], Batch [20/102], Avg Loss (last 10): 0.6616
Epoch [10/50], Batch [30/102], Avg Loss (last 10): 0.6177
Epoch [10/50], Batch [40/102], Avg Loss (last 10): 0.6869
Epoch [10/50], Batch [50/102], Avg Loss (last 10): 0.6679
Epoch [10/50], Batch [60/102], Avg Loss (last 10): 0.6171
Epoch [10/50], Batch [70/102], Avg Loss (last 10): 0.6232
Epoch [10/50], Batch [80/102], Avg Loss (last 10): 0.6487
Epoch [10/50], Batch [90/102], Avg Loss (last 10): 0.6488
Epoch [10/50], Batch [100/102], Avg Loss (last 10): 0.6290
Epoch [10/50] Average Training Loss: 0.6368
Epoch [11/50], Batch [10/102], Avg Loss (last 10): 0.5892
Epoch [11/50], Batch [20/102], Avg Loss (last 10): 0.6352
Epoch [11/50], Batch [30/102], Avg Loss (last 10): 0.6147
Epoch [11/50], Batch [40/102], Avg Loss (last 10): 0.6346
Epoch [11/50], Batch [50/102], Avg Loss (last 10): 0.6753
Epoch [11/50], Batch [60/102], Avg Loss (last 10): 0.6481
Epoch [11/50], Batch [70/102], Avg Loss (last 10): 0.6892
Epoch [11/50], Batch [80/102], Avg Loss (last 10): 0.6205
Epoch [11/50], Batch [90/102], Avg Loss (last 10): 0.6756
Epoch [11/50], Batch [100/102], Avg Loss (last 10): 0.5649
Epoch [11/50] Average Training Loss: 0.6352
Epoch [12/50], Batch [10/102], Avg Loss (last 10): 0.6340
Epoch [12/50], Batch [20/102], Avg Loss (last 10): 0.6266
Epoch [12/50], Batch [30/102], Avg Loss (last 10): 0.6194
Epoch [12/50], Batch [40/102], Avg Loss (last 10): 0.5759
Epoch [12/50], Batch [50/102], Avg Loss (last 10): 0.6258
Epoch [12/50], Batch [60/102], Avg Loss (last 10): 0.6695
Epoch [12/50], Batch [70/102], Avg Loss (last 10): 0.6329
Epoch [12/50], Batch [80/102], Avg Loss (last 10): 0.6401
Epoch [12/50], Batch [90/102], Avg Loss (last 10): 0.6401
Epoch [12/50], Batch [100/102], Avg Loss (last 10): 0.6996
Epoch [12/50] Average Training Loss: 0.6340
Epoch [13/50], Batch [10/102], Avg Loss (last 10): 0.5581
Epoch [13/50], Batch [20/102], Avg Loss (last 10): 0.6325
Epoch [13/50], Batch [30/102], Avg Loss (last 10): 0.5944
Epoch [13/50], Batch [40/102], Avg Loss (last 10): 0.6474
Epoch [13/50], Batch [50/102], Avg Loss (last 10): 0.7089
Epoch [13/50], Batch [60/102], Avg Loss (last 10): 0.6321
Epoch [13/50], Batch [70/102], Avg Loss (last 10): 0.6398
Epoch [13/50], Batch [80/102], Avg Loss (last 10): 0.6165
Epoch [13/50], Batch [90/102], Avg Loss (last 10): 0.6862
Epoch [13/50], Batch [100/102], Avg Loss (last 10): 0.6397
Epoch [13/50] Average Training Loss: 0.6331
Epoch [14/50], Batch [10/102], Avg Loss (last 10): 0.5928
Epoch [14/50], Batch [20/102], Avg Loss (last 10): 0.6870
Epoch [14/50], Batch [30/102], Avg Loss (last 10): 0.6711
Epoch [14/50], Batch [40/102], Avg Loss (last 10): 0.6081
Epoch [14/50], Batch [50/102], Avg Loss (last 10): 0.6475
Epoch [14/50], Batch [60/102], Avg Loss (last 10): 0.5998
Epoch [14/50], Batch [70/102], Avg Loss (last 10): 0.6236
Epoch [14/50], Batch [80/102], Avg Loss (last 10): 0.6154
Epoch [14/50], Batch [90/102], Avg Loss (last 10): 0.6636
Epoch [14/50], Batch [100/102], Avg Loss (last 10): 0.6395
Epoch [14/50] Average Training Loss: 0.6322
Epoch [15/50], Batch [10/102], Avg Loss (last 10): 0.5746
Epoch [15/50], Batch [20/102], Avg Loss (last 10): 0.6475
Epoch [15/50], Batch [30/102], Avg Loss (last 10): 0.7049
Epoch [15/50], Batch [40/102], Avg Loss (last 10): 0.6476
Epoch [15/50], Batch [50/102], Avg Loss (last 10): 0.6394
Epoch [15/50], Batch [60/102], Avg Loss (last 10): 0.5655
Epoch [15/50], Batch [70/102], Avg Loss (last 10): 0.6643
Epoch [15/50], Batch [80/102], Avg Loss (last 10): 0.6311
Epoch [15/50], Batch [90/102], Avg Loss (last 10): 0.6560
Epoch [15/50], Batch [100/102], Avg Loss (last 10): 0.6143
Epoch [15/50] Average Training Loss: 0.6318
Epoch [16/50], Batch [10/102], Avg Loss (last 10): 0.6478
Epoch [16/50], Batch [20/102], Avg Loss (last 10): 0.6057
Epoch [16/50], Batch [30/102], Avg Loss (last 10): 0.6139
Epoch [16/50], Batch [40/102], Avg Loss (last 10): 0.6564
Epoch [16/50], Batch [50/102], Avg Loss (last 10): 0.5968
Epoch [16/50], Batch [60/102], Avg Loss (last 10): 0.7251
Epoch [16/50], Batch [70/102], Avg Loss (last 10): 0.5193
Epoch [16/50], Batch [80/102], Avg Loss (last 10): 0.6223
Epoch [16/50], Batch [90/102], Avg Loss (last 10): 0.6568
Epoch [16/50], Batch [100/102], Avg Loss (last 10): 0.7089
Epoch [16/50] Average Training Loss: 0.6316
Epoch [17/50], Batch [10/102], Avg Loss (last 10): 0.5872
Epoch [17/50], Batch [20/102], Avg Loss (last 10): 0.7093
Epoch [17/50], Batch [30/102], Avg Loss (last 10): 0.6307
Epoch [17/50], Batch [40/102], Avg Loss (last 10): 0.6307
Epoch [17/50], Batch [50/102], Avg Loss (last 10): 0.6570
Epoch [17/50], Batch [60/102], Avg Loss (last 10): 0.6131
Epoch [17/50], Batch [70/102], Avg Loss (last 10): 0.5778
Epoch [17/50], Batch [80/102], Avg Loss (last 10): 0.6572
Epoch [17/50], Batch [90/102], Avg Loss (last 10): 0.6217
Epoch [17/50], Batch [100/102], Avg Loss (last 10): 0.6484
Epoch [17/50] Average Training Loss: 0.6375
Epoch [18/50], Batch [10/102], Avg Loss (last 10): 0.6663
Epoch [18/50], Batch [20/102], Avg Loss (last 10): 0.6931
Epoch [18/50], Batch [30/102], Avg Loss (last 10): 0.6840
Epoch [18/50], Batch [40/102], Avg Loss (last 10): 0.6129
Epoch [18/50], Batch [50/102], Avg Loss (last 10): 0.5951
Epoch [18/50], Batch [60/102], Avg Loss (last 10): 0.6395
Epoch [18/50], Batch [70/102], Avg Loss (last 10): 0.5947
Epoch [18/50], Batch [80/102], Avg Loss (last 10): 0.6125
Epoch [18/50], Batch [90/102], Avg Loss (last 10): 0.6124
Epoch [18/50], Batch [100/102], Avg Loss (last 10): 0.5940
Epoch [18/50] Average Training Loss: 0.6375
Epoch [19/50], Batch [10/102], Avg Loss (last 10): 0.6307
Epoch [19/50], Batch [20/102], Avg Loss (last 10): 0.6214
Epoch [19/50], Batch [30/102], Avg Loss (last 10): 0.5937
Epoch [19/50], Batch [40/102], Avg Loss (last 10): 0.6213
Epoch [19/50], Batch [50/102], Avg Loss (last 10): 0.6492
Epoch [19/50], Batch [60/102], Avg Loss (last 10): 0.5652
Epoch [19/50], Batch [70/102], Avg Loss (last 10): 0.6494
Epoch [19/50], Batch [80/102], Avg Loss (last 10): 0.6777
Epoch [19/50], Batch [90/102], Avg Loss (last 10): 0.7245
Epoch [19/50], Batch [100/102], Avg Loss (last 10): 0.6025
Epoch [19/50] Average Training Loss: 0.6315
Epoch [20/50], Batch [10/102], Avg Loss (last 10): 0.6400
Epoch [20/50], Batch [20/102], Avg Loss (last 10): 0.6119
Epoch [20/50], Batch [30/102], Avg Loss (last 10): 0.6024
Epoch [20/50], Batch [40/102], Avg Loss (last 10): 0.5548
Epoch [20/50], Batch [50/102], Avg Loss (last 10): 0.6116
Epoch [20/50], Batch [60/102], Avg Loss (last 10): 0.5923
Epoch [20/50], Batch [70/102], Avg Loss (last 10): 0.6596
Epoch [20/50], Batch [80/102], Avg Loss (last 10): 0.7370
Epoch [20/50], Batch [90/102], Avg Loss (last 10): 0.6598
Epoch [20/50], Batch [100/102], Avg Loss (last 10): 0.6597
Epoch [20/50] Average Training Loss: 0.6383
Epoch [21/50], Batch [10/102], Avg Loss (last 10): 0.5826
Epoch [21/50], Batch [20/102], Avg Loss (last 10): 0.6211
Epoch [21/50], Batch [30/102], Avg Loss (last 10): 0.5824
Epoch [21/50], Batch [40/102], Avg Loss (last 10): 0.6407
Epoch [21/50], Batch [50/102], Avg Loss (last 10): 0.6698
Epoch [21/50], Batch [60/102], Avg Loss (last 10): 0.6406
Epoch [21/50], Batch [70/102], Avg Loss (last 10): 0.6795
Epoch [21/50], Batch [80/102], Avg Loss (last 10): 0.6309
Epoch [21/50], Batch [90/102], Avg Loss (last 10): 0.6308
Epoch [21/50], Batch [100/102], Avg Loss (last 10): 0.6113
Epoch [21/50] Average Training Loss: 0.6384
Epoch [22/50], Batch [10/102], Avg Loss (last 10): 0.5918
Epoch [22/50], Batch [20/102], Avg Loss (last 10): 0.6504
Epoch [22/50], Batch [30/102], Avg Loss (last 10): 0.6798
Epoch [22/50], Batch [40/102], Avg Loss (last 10): 0.6895
Epoch [22/50], Batch [50/102], Avg Loss (last 10): 0.5527
Epoch [22/50], Batch [60/102], Avg Loss (last 10): 0.6309
Epoch [22/50], Batch [70/102], Avg Loss (last 10): 0.6309
Epoch [22/50], Batch [80/102], Avg Loss (last 10): 0.5719
Epoch [22/50], Batch [90/102], Avg Loss (last 10): 0.6606
Epoch [22/50], Batch [100/102], Avg Loss (last 10): 0.6903
Epoch [22/50] Average Training Loss: 0.6317
Epoch [23/50], Batch [10/102], Avg Loss (last 10): 0.6013
Epoch [23/50], Batch [20/102], Avg Loss (last 10): 0.5813
Epoch [23/50], Batch [30/102], Avg Loss (last 10): 0.7109
Epoch [23/50], Batch [40/102], Avg Loss (last 10): 0.6610
Epoch [23/50], Batch [50/102], Avg Loss (last 10): 0.6808
Epoch [23/50], Batch [60/102], Avg Loss (last 10): 0.5813
Epoch [23/50], Batch [70/102], Avg Loss (last 10): 0.6510
Epoch [23/50], Batch [80/102], Avg Loss (last 10): 0.6610
Epoch [23/50], Batch [90/102], Avg Loss (last 10): 0.5812
Epoch [23/50], Batch [100/102], Avg Loss (last 10): 0.6211
Epoch [23/50] Average Training Loss: 0.6388
Epoch [24/50], Batch [10/102], Avg Loss (last 10): 0.6512
Epoch [24/50], Batch [20/102], Avg Loss (last 10): 0.6512
Epoch [24/50], Batch [30/102], Avg Loss (last 10): 0.6412
Epoch [24/50], Batch [40/102], Avg Loss (last 10): 0.6815
Epoch [24/50], Batch [50/102], Avg Loss (last 10): 0.6211
Epoch [24/50], Batch [60/102], Avg Loss (last 10): 0.6412
Epoch [24/50], Batch [70/102], Avg Loss (last 10): 0.6412
Epoch [24/50], Batch [80/102], Avg Loss (last 10): 0.6211
Epoch [24/50], Batch [90/102], Avg Loss (last 10): 0.6412
Epoch [24/50], Batch [100/102], Avg Loss (last 10): 0.5506
Epoch [24/50] Average Training Loss: 0.6319
Epoch [25/50], Batch [10/102], Avg Loss (last 10): 0.6009
Epoch [25/50], Batch [20/102], Avg Loss (last 10): 0.6824
Epoch [25/50], Batch [30/102], Avg Loss (last 10): 0.6110
Epoch [25/50], Batch [40/102], Avg Loss (last 10): 0.6110
Epoch [25/50], Batch [50/102], Avg Loss (last 10): 0.6110
Epoch [25/50], Batch [60/102], Avg Loss (last 10): 0.6934
Epoch [25/50], Batch [70/102], Avg Loss (last 10): 0.6419
Epoch [25/50], Batch [80/102], Avg Loss (last 10): 0.6727
Epoch [25/50], Batch [90/102], Avg Loss (last 10): 0.6212
Epoch [25/50], Batch [100/102], Avg Loss (last 10): 0.6109
Epoch [25/50] Average Training Loss: 0.6323
Epoch [26/50], Batch [10/102], Avg Loss (last 10): 0.6007
Epoch [26/50], Batch [20/102], Avg Loss (last 10): 0.6006
Epoch [26/50], Batch [30/102], Avg Loss (last 10): 0.6524
Epoch [26/50], Batch [40/102], Avg Loss (last 10): 0.6628
Epoch [26/50], Batch [50/102], Avg Loss (last 10): 0.6420
Epoch [26/50], Batch [60/102], Avg Loss (last 10): 0.6213
Epoch [26/50], Batch [70/102], Avg Loss (last 10): 0.6733
Epoch [26/50], Batch [80/102], Avg Loss (last 10): 0.6836
Epoch [26/50], Batch [90/102], Avg Loss (last 10): 0.5694
Epoch [26/50], Batch [100/102], Avg Loss (last 10): 0.6317
Epoch [26/50] Average Training Loss: 0.6325
Epoch [27/50], Batch [10/102], Avg Loss (last 10): 0.5797
Epoch [27/50], Batch [20/102], Avg Loss (last 10): 0.6630
Epoch [27/50], Batch [30/102], Avg Loss (last 10): 0.7046
Epoch [27/50], Batch [40/102], Avg Loss (last 10): 0.6005
Epoch [27/50], Batch [50/102], Avg Loss (last 10): 0.6005
Epoch [27/50], Batch [60/102], Avg Loss (last 10): 0.5900
Epoch [27/50], Batch [70/102], Avg Loss (last 10): 0.7154
Epoch [27/50], Batch [80/102], Avg Loss (last 10): 0.5900
Epoch [27/50], Batch [90/102], Avg Loss (last 10): 0.5795
Epoch [27/50], Batch [100/102], Avg Loss (last 10): 0.7260
Epoch [27/50] Average Training Loss: 0.6398
Epoch [28/50], Batch [10/102], Avg Loss (last 10): 0.6841
Epoch [28/50], Batch [20/102], Avg Loss (last 10): 0.6736
Epoch [28/50], Batch [30/102], Avg Loss (last 10): 0.6213
Epoch [28/50], Batch [40/102], Avg Loss (last 10): 0.6526
Epoch [28/50], Batch [50/102], Avg Loss (last 10): 0.6526
Epoch [28/50], Batch [60/102], Avg Loss (last 10): 0.5482
Epoch [28/50], Batch [70/102], Avg Loss (last 10): 0.6632
Epoch [28/50], Batch [80/102], Avg Loss (last 10): 0.6213
Epoch [28/50], Batch [90/102], Avg Loss (last 10): 0.6318
Epoch [28/50], Batch [100/102], Avg Loss (last 10): 0.6109
Epoch [28/50] Average Training Loss: 0.6326
Epoch [29/50], Batch [10/102], Avg Loss (last 10): 0.6738
Epoch [29/50], Batch [20/102], Avg Loss (last 10): 0.6004
Epoch [29/50], Batch [30/102], Avg Loss (last 10): 0.6843
Epoch [29/50], Batch [40/102], Avg Loss (last 10): 0.5479
Epoch [29/50], Batch [50/102], Avg Loss (last 10): 0.5794
Epoch [29/50], Batch [60/102], Avg Loss (last 10): 0.6425
Epoch [29/50], Batch [70/102], Avg Loss (last 10): 0.7058
Epoch [29/50], Batch [80/102], Avg Loss (last 10): 0.6214
Epoch [29/50], Batch [90/102], Avg Loss (last 10): 0.6425
Epoch [29/50], Batch [100/102], Avg Loss (last 10): 0.6635
Epoch [29/50] Average Training Loss: 0.6328
Epoch [30/50], Batch [10/102], Avg Loss (last 10): 0.6319
Epoch [30/50], Batch [20/102], Avg Loss (last 10): 0.6425
Epoch [30/50], Batch [30/102], Avg Loss (last 10): 0.6530
Epoch [30/50], Batch [40/102], Avg Loss (last 10): 0.6319
Epoch [30/50], Batch [50/102], Avg Loss (last 10): 0.7055
Epoch [30/50], Batch [60/102], Avg Loss (last 10): 0.6004
Epoch [30/50], Batch [70/102], Avg Loss (last 10): 0.6635
Epoch [30/50], Batch [80/102], Avg Loss (last 10): 0.5898
Epoch [30/50], Batch [90/102], Avg Loss (last 10): 0.6109
Epoch [30/50], Batch [100/102], Avg Loss (last 10): 0.6214
Epoch [30/50] Average Training Loss: 0.6328
Epoch [31/50], Batch [10/102], Avg Loss (last 10): 0.6320
Epoch [31/50], Batch [20/102], Avg Loss (last 10): 0.6847
Epoch [31/50], Batch [30/102], Avg Loss (last 10): 0.6109
Epoch [31/50], Batch [40/102], Avg Loss (last 10): 0.5898
Epoch [31/50], Batch [50/102], Avg Loss (last 10): 0.6637
Epoch [31/50], Batch [60/102], Avg Loss (last 10): 0.6109
Epoch [31/50], Batch [70/102], Avg Loss (last 10): 0.6109
Epoch [31/50], Batch [80/102], Avg Loss (last 10): 0.6320
Epoch [31/50], Batch [90/102], Avg Loss (last 10): 0.7061
Epoch [31/50], Batch [100/102], Avg Loss (last 10): 0.6109
Epoch [31/50] Average Training Loss: 0.6328
Epoch [32/50], Batch [10/102], Avg Loss (last 10): 0.6320
Epoch [32/50], Batch [20/102], Avg Loss (last 10): 0.6532
Epoch [32/50], Batch [30/102], Avg Loss (last 10): 0.6743
Epoch [32/50], Batch [40/102], Avg Loss (last 10): 0.6003
Epoch [32/50], Batch [50/102], Avg Loss (last 10): 0.6320
Epoch [32/50], Batch [60/102], Avg Loss (last 10): 0.6638
Epoch [32/50], Batch [70/102], Avg Loss (last 10): 0.6214
Epoch [32/50], Batch [80/102], Avg Loss (last 10): 0.5791
Epoch [32/50], Batch [90/102], Avg Loss (last 10): 0.6427
Epoch [32/50], Batch [100/102], Avg Loss (last 10): 0.6427
Epoch [32/50] Average Training Loss: 0.6401
Epoch [33/50], Batch [10/102], Avg Loss (last 10): 0.6533
Epoch [33/50], Batch [20/102], Avg Loss (last 10): 0.6851
Epoch [33/50], Batch [30/102], Avg Loss (last 10): 0.6427
Epoch [33/50], Batch [40/102], Avg Loss (last 10): 0.6321
Epoch [33/50], Batch [50/102], Avg Loss (last 10): 0.6215
Epoch [33/50], Batch [60/102], Avg Loss (last 10): 0.5897
Epoch [33/50], Batch [70/102], Avg Loss (last 10): 0.5579
Epoch [33/50], Batch [80/102], Avg Loss (last 10): 0.6852
Epoch [33/50], Batch [90/102], Avg Loss (last 10): 0.6427
Epoch [33/50], Batch [100/102], Avg Loss (last 10): 0.6639
Epoch [33/50] Average Training Loss: 0.6329
Epoch [34/50], Batch [10/102], Avg Loss (last 10): 0.6109
Epoch [34/50], Batch [20/102], Avg Loss (last 10): 0.5684
Epoch [34/50], Batch [30/102], Avg Loss (last 10): 0.6428
Epoch [34/50], Batch [40/102], Avg Loss (last 10): 0.6640
Epoch [34/50], Batch [50/102], Avg Loss (last 10): 0.6640
Epoch [34/50], Batch [60/102], Avg Loss (last 10): 0.6746
Epoch [34/50], Batch [70/102], Avg Loss (last 10): 0.5578
Epoch [34/50], Batch [80/102], Avg Loss (last 10): 0.6853
Epoch [34/50], Batch [90/102], Avg Loss (last 10): 0.6640
Epoch [34/50], Batch [100/102], Avg Loss (last 10): 0.6534
Epoch [34/50] Average Training Loss: 0.6330
Epoch [35/50], Batch [10/102], Avg Loss (last 10): 0.6002
Epoch [35/50], Batch [20/102], Avg Loss (last 10): 0.6428
Epoch [35/50], Batch [30/102], Avg Loss (last 10): 0.6854
Epoch [35/50], Batch [40/102], Avg Loss (last 10): 0.6747
Epoch [35/50], Batch [50/102], Avg Loss (last 10): 0.6109
Epoch [35/50], Batch [60/102], Avg Loss (last 10): 0.6215
Epoch [35/50], Batch [70/102], Avg Loss (last 10): 0.5364
Epoch [35/50], Batch [80/102], Avg Loss (last 10): 0.6428
Epoch [35/50], Batch [90/102], Avg Loss (last 10): 0.6429
Epoch [35/50], Batch [100/102], Avg Loss (last 10): 0.6962
Epoch [35/50] Average Training Loss: 0.6403
Epoch [36/50], Batch [10/102], Avg Loss (last 10): 0.6535
Epoch [36/50], Batch [20/102], Avg Loss (last 10): 0.6641
Epoch [36/50], Batch [30/102], Avg Loss (last 10): 0.5683
Epoch [36/50], Batch [40/102], Avg Loss (last 10): 0.5576
Epoch [36/50], Batch [50/102], Avg Loss (last 10): 0.6962
Epoch [36/50], Batch [60/102], Avg Loss (last 10): 0.6322
Epoch [36/50], Batch [70/102], Avg Loss (last 10): 0.6322
Epoch [36/50], Batch [80/102], Avg Loss (last 10): 0.6109
Epoch [36/50], Batch [90/102], Avg Loss (last 10): 0.7070
Epoch [36/50], Batch [100/102], Avg Loss (last 10): 0.6216
Epoch [36/50] Average Training Loss: 0.6404
Epoch [37/50], Batch [10/102], Avg Loss (last 10): 0.5468
Epoch [37/50], Batch [20/102], Avg Loss (last 10): 0.6429
Epoch [37/50], Batch [30/102], Avg Loss (last 10): 0.6536
Epoch [37/50], Batch [40/102], Avg Loss (last 10): 0.6750
Epoch [37/50], Batch [50/102], Avg Loss (last 10): 0.6002
Epoch [37/50], Batch [60/102], Avg Loss (last 10): 0.6323
Epoch [37/50], Batch [70/102], Avg Loss (last 10): 0.6536
Epoch [37/50], Batch [80/102], Avg Loss (last 10): 0.6430
Epoch [37/50], Batch [90/102], Avg Loss (last 10): 0.6323
Epoch [37/50], Batch [100/102], Avg Loss (last 10): 0.6750
Epoch [37/50] Average Training Loss: 0.6404
Epoch [38/50], Batch [10/102], Avg Loss (last 10): 0.6536
Epoch [38/50], Batch [20/102], Avg Loss (last 10): 0.5788
Epoch [38/50], Batch [30/102], Avg Loss (last 10): 0.6536
Epoch [38/50], Batch [40/102], Avg Loss (last 10): 0.6429
Epoch [38/50], Batch [50/102], Avg Loss (last 10): 0.6109
Epoch [38/50], Batch [60/102], Avg Loss (last 10): 0.6429
Epoch [38/50], Batch [70/102], Avg Loss (last 10): 0.6536
Epoch [38/50], Batch [80/102], Avg Loss (last 10): 0.6322
Epoch [38/50], Batch [90/102], Avg Loss (last 10): 0.6109
Epoch [38/50], Batch [100/102], Avg Loss (last 10): 0.6750
Epoch [38/50] Average Training Loss: 0.6331
Epoch [39/50], Batch [10/102], Avg Loss (last 10): 0.5895
Epoch [39/50], Batch [20/102], Avg Loss (last 10): 0.6750
Epoch [39/50], Batch [30/102], Avg Loss (last 10): 0.6536
Epoch [39/50], Batch [40/102], Avg Loss (last 10): 0.7178
Epoch [39/50], Batch [50/102], Avg Loss (last 10): 0.6216
Epoch [39/50], Batch [60/102], Avg Loss (last 10): 0.6216
Epoch [39/50], Batch [70/102], Avg Loss (last 10): 0.6750
Epoch [39/50], Batch [80/102], Avg Loss (last 10): 0.5788
Epoch [39/50], Batch [90/102], Avg Loss (last 10): 0.6536
Epoch [39/50], Batch [100/102], Avg Loss (last 10): 0.5574
Epoch [39/50] Average Training Loss: 0.6404
Epoch [40/50], Batch [10/102], Avg Loss (last 10): 0.6644
Epoch [40/50], Batch [20/102], Avg Loss (last 10): 0.6323
Epoch [40/50], Batch [30/102], Avg Loss (last 10): 0.6430
Epoch [40/50], Batch [40/102], Avg Loss (last 10): 0.6751
Epoch [40/50], Batch [50/102], Avg Loss (last 10): 0.6965
Epoch [40/50], Batch [60/102], Avg Loss (last 10): 0.6430
Epoch [40/50], Batch [70/102], Avg Loss (last 10): 0.5895
Epoch [40/50], Batch [80/102], Avg Loss (last 10): 0.5467
Epoch [40/50], Batch [90/102], Avg Loss (last 10): 0.6858
Epoch [40/50], Batch [100/102], Avg Loss (last 10): 0.6002
Epoch [40/50] Average Training Loss: 0.6331
Epoch [41/50], Batch [10/102], Avg Loss (last 10): 0.6216
Epoch [41/50], Batch [20/102], Avg Loss (last 10): 0.7610
Epoch [41/50], Batch [30/102], Avg Loss (last 10): 0.5894
Epoch [41/50], Batch [40/102], Avg Loss (last 10): 0.6323
Epoch [41/50], Batch [50/102], Avg Loss (last 10): 0.6109
Epoch [41/50], Batch [60/102], Avg Loss (last 10): 0.6323
Epoch [41/50], Batch [70/102], Avg Loss (last 10): 0.6430
Epoch [41/50], Batch [80/102], Avg Loss (last 10): 0.5680
Epoch [41/50], Batch [90/102], Avg Loss (last 10): 0.6323
Epoch [41/50], Batch [100/102], Avg Loss (last 10): 0.6431
Epoch [41/50] Average Training Loss: 0.6332
Epoch [42/50], Batch [10/102], Avg Loss (last 10): 0.7933
Epoch [42/50], Batch [20/102], Avg Loss (last 10): 0.6216
Epoch [42/50], Batch [30/102], Avg Loss (last 10): 0.6002
Epoch [42/50], Batch [40/102], Avg Loss (last 10): 0.6538
Epoch [42/50], Batch [50/102], Avg Loss (last 10): 0.7074
Epoch [42/50], Batch [60/102], Avg Loss (last 10): 0.5251
Epoch [42/50], Batch [70/102], Avg Loss (last 10): 0.6002
Epoch [42/50], Batch [80/102], Avg Loss (last 10): 0.6324
Epoch [42/50], Batch [90/102], Avg Loss (last 10): 0.6324
Epoch [42/50], Batch [100/102], Avg Loss (last 10): 0.6109
Epoch [42/50] Average Training Loss: 0.6332
Epoch [43/50], Batch [10/102], Avg Loss (last 10): 0.6109
Epoch [43/50], Batch [20/102], Avg Loss (last 10): 0.6431
Epoch [43/50], Batch [30/102], Avg Loss (last 10): 0.7291
Epoch [43/50], Batch [40/102], Avg Loss (last 10): 0.5464
Epoch [43/50], Batch [50/102], Avg Loss (last 10): 0.6539
Epoch [43/50], Batch [60/102], Avg Loss (last 10): 0.6001
Epoch [43/50], Batch [70/102], Avg Loss (last 10): 0.6001
Epoch [43/50], Batch [80/102], Avg Loss (last 10): 0.6647
Epoch [43/50], Batch [90/102], Avg Loss (last 10): 0.6324
Epoch [43/50], Batch [100/102], Avg Loss (last 10): 0.6754
Epoch [43/50] Average Training Loss: 0.6332
Epoch [44/50], Batch [10/102], Avg Loss (last 10): 0.5463
Epoch [44/50], Batch [20/102], Avg Loss (last 10): 0.6539
Epoch [44/50], Batch [30/102], Avg Loss (last 10): 0.5678
Epoch [44/50], Batch [40/102], Avg Loss (last 10): 0.6539
Epoch [44/50], Batch [50/102], Avg Loss (last 10): 0.6755
Epoch [44/50], Batch [60/102], Avg Loss (last 10): 0.7508
Epoch [44/50], Batch [70/102], Avg Loss (last 10): 0.6432
Epoch [44/50], Batch [80/102], Avg Loss (last 10): 0.6109
Epoch [44/50], Batch [90/102], Avg Loss (last 10): 0.6754
Epoch [44/50], Batch [100/102], Avg Loss (last 10): 0.5571
Epoch [44/50] Average Training Loss: 0.6333
Epoch [45/50], Batch [10/102], Avg Loss (last 10): 0.5571
Epoch [45/50], Batch [20/102], Avg Loss (last 10): 0.6324
Epoch [45/50], Batch [30/102], Avg Loss (last 10): 0.6540
Epoch [45/50], Batch [40/102], Avg Loss (last 10): 0.6862
Epoch [45/50], Batch [50/102], Avg Loss (last 10): 0.6324
Epoch [45/50], Batch [60/102], Avg Loss (last 10): 0.6109
Epoch [45/50], Batch [70/102], Avg Loss (last 10): 0.6324
Epoch [45/50], Batch [80/102], Avg Loss (last 10): 0.7616
Epoch [45/50], Batch [90/102], Avg Loss (last 10): 0.5786
Epoch [45/50], Batch [100/102], Avg Loss (last 10): 0.6109
Epoch [45/50] Average Training Loss: 0.6407
Epoch [46/50], Batch [10/102], Avg Loss (last 10): 0.6863
Epoch [46/50], Batch [20/102], Avg Loss (last 10): 0.6432
Epoch [46/50], Batch [30/102], Avg Loss (last 10): 0.6001
Epoch [46/50], Batch [40/102], Avg Loss (last 10): 0.6540
Epoch [46/50], Batch [50/102], Avg Loss (last 10): 0.5786
Epoch [46/50], Batch [60/102], Avg Loss (last 10): 0.5786
Epoch [46/50], Batch [70/102], Avg Loss (last 10): 0.6432
Epoch [46/50], Batch [80/102], Avg Loss (last 10): 0.6109
Epoch [46/50], Batch [90/102], Avg Loss (last 10): 0.6648
Epoch [46/50], Batch [100/102], Avg Loss (last 10): 0.7078
Epoch [46/50] Average Training Loss: 0.6407
Epoch [47/50], Batch [10/102], Avg Loss (last 10): 0.5355
Epoch [47/50], Batch [20/102], Avg Loss (last 10): 0.6217
Epoch [47/50], Batch [30/102], Avg Loss (last 10): 0.6109
Epoch [47/50], Batch [40/102], Avg Loss (last 10): 0.6217
Epoch [47/50], Batch [50/102], Avg Loss (last 10): 0.6109
Epoch [47/50], Batch [60/102], Avg Loss (last 10): 0.6325
Epoch [47/50], Batch [70/102], Avg Loss (last 10): 0.6109
Epoch [47/50], Batch [80/102], Avg Loss (last 10): 0.6432
Epoch [47/50], Batch [90/102], Avg Loss (last 10): 0.6540
Epoch [47/50], Batch [100/102], Avg Loss (last 10): 0.8266
Epoch [47/50] Average Training Loss: 0.6333
Epoch [48/50], Batch [10/102], Avg Loss (last 10): 0.6432
Epoch [48/50], Batch [20/102], Avg Loss (last 10): 0.6109
Epoch [48/50], Batch [30/102], Avg Loss (last 10): 0.7079
Epoch [48/50], Batch [40/102], Avg Loss (last 10): 0.6540
Epoch [48/50], Batch [50/102], Avg Loss (last 10): 0.5786
Epoch [48/50], Batch [60/102], Avg Loss (last 10): 0.6540
Epoch [48/50], Batch [70/102], Avg Loss (last 10): 0.5893
Epoch [48/50], Batch [80/102], Avg Loss (last 10): 0.6001
Epoch [48/50], Batch [90/102], Avg Loss (last 10): 0.7618
Epoch [48/50], Batch [100/102], Avg Loss (last 10): 0.5570
Epoch [48/50] Average Training Loss: 0.6407
Epoch [49/50], Batch [10/102], Avg Loss (last 10): 0.6109
Epoch [49/50], Batch [20/102], Avg Loss (last 10): 0.5893
Epoch [49/50], Batch [30/102], Avg Loss (last 10): 0.6432
Epoch [49/50], Batch [40/102], Avg Loss (last 10): 0.6325
Epoch [49/50], Batch [50/102], Avg Loss (last 10): 0.6325
Epoch [49/50], Batch [60/102], Avg Loss (last 10): 0.6325
Epoch [49/50], Batch [70/102], Avg Loss (last 10): 0.6972
Epoch [49/50], Batch [80/102], Avg Loss (last 10): 0.5893
Epoch [49/50], Batch [90/102], Avg Loss (last 10): 0.6325
Epoch [49/50], Batch [100/102], Avg Loss (last 10): 0.7080
Epoch [49/50] Average Training Loss: 0.6333
Epoch [50/50], Batch [10/102], Avg Loss (last 10): 0.7296
Epoch [50/50], Batch [20/102], Avg Loss (last 10): 0.5677
Epoch [50/50], Batch [30/102], Avg Loss (last 10): 0.5569
Epoch [50/50], Batch [40/102], Avg Loss (last 10): 0.5461
Epoch [50/50], Batch [50/102], Avg Loss (last 10): 0.6649
Epoch [50/50], Batch [60/102], Avg Loss (last 10): 0.6649
Epoch [50/50], Batch [70/102], Avg Loss (last 10): 0.5893
Epoch [50/50], Batch [80/102], Avg Loss (last 10): 0.7404
Epoch [50/50], Batch [90/102], Avg Loss (last 10): 0.6433
Epoch [50/50], Batch [100/102], Avg Loss (last 10): 0.6325
Epoch [50/50] Average Training Loss: 0.6333
